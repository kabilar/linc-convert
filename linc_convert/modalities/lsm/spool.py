# stdlib
import ast
import math
import re
import time
from collections import namedtuple, defaultdict
from configparser import ConfigParser
from glob import glob

# externals
import cyclopts
import nibabel as nib
import numpy as np
import scipy.io
import zarr
from niizarr._nii2zarr import write_ome_metadata
from niizarr import default_nifti_header, write_nifti_header

# internals
from linc_convert import utils
from linc_convert.modalities.lsm.cli import lsm
from linc_convert.utils.math import ceildiv
from linc_convert.utils.orientation import center_affine, orientation_to_affine
from linc_convert.utils.zarr.compressor import make_compressor
from linc_convert.utils.zarr.zarr_config import ZarrConfig
from linc_convert.utils.zarr.generate_pyramid import generate_pyramid

spool = cyclopts.App(name="spool", help_format="markdown")
lsm.command(spool)

"""
Convert a collection of tiff files generated by the LSM pipeline into a Zarr.

Example input files can be found at
https://lincbrain.org/dandiset/000010/draft/files?location=sourcedata%252Fderivatives
"""


@spool.default
def convert(
        inp: str,
        *,
        out: str,
        zarr_config: ZarrConfig = None,
        overlap: int = 30,
        max_load: int = 512,
        orientation: str = "coronal",
        center: bool = True,
        voxel_size: list[float] = (1, 1, 1),
        **kwargs
) -> None:
    """
    Convert a collection of tiff files generated by the LSM pipeline into ZARR.

    Orientation
    -----------
    The anatomical orientation of the slice is given in terms of RAS axes.

    It is a combination of two letters from the set
    `{"L", "R", "A", "P", "I", "S"}`, where

    * the first letter corresponds to the horizontal dimension and
        indicates the anatomical meaning of the _right_ of the jp2 image,
    * the second letter corresponds to the vertical dimension and
        indicates the anatomical meaning of the _bottom_ of the jp2 image.

    We also provide the aliases

    * `"coronal"` == `"LI"`
    * `"axial"` == `"LP"`
    * `"sagittal"` == `"PI"`

    The orientation flag is only useful when converting to nifti-zarr.

    Parameters
    ----------
    inp
        Path to the root directory, which contains a collection of
        subfolders named `*_z{:02d}_y{:02d}*`, each containing a
        collection of files named `*_plane{:03d}_c{:d}.tiff`.
    out
        Path to the output Zarr directory [<INP>.ome.zarr]
    max_load
        Maximum input chunk size when building pyramid
    orientation
        Orientation of the slice
    center
        Set RAS[0, 0, 0] at FOV center
    voxel_size
        Voxel size along the X, Y and Z dimension, in micron.
    """
    zarr_config = utils.zarr.zarr_config.update(zarr_config, **kwargs)
    chunk: int = zarr_config.chunk[0]
    compressor: str = zarr_config.compressor
    compressor_opt: str = zarr_config.compressor_opt
    nii: bool = zarr_config.nii

    if isinstance(compressor_opt, str):
        compressor_opt = ast.literal_eval(compressor_opt)

    CHUNK_PATTERN = re.compile(
        r"^(?P<prefix>\w*)"
        r"_run(?P<run>[0-9]+)"
        r"_?"
        r"_y(?P<y>[0-9]+)"
        r"_z(?P<z>[0-9]+)"
        r"(?P<suffix>\w*)$"
    )

    all_tiles_folders_names = sorted(glob(os.path.join(inp, "*_y*_z*_HR/")))
    if not all_tiles_folders_names:
        raise ValueError("No tile folder found")

    all_tiles_info = []
    tiles_info_by_index = {}
    TileInfo = namedtuple(
        "TileInfo",
        ["prefix", "run", "y", "z", "suffix", "filename", "reader"]
    )
    for tile_folder_name in all_tiles_folders_names:
        if tile_folder_name.endswith(r"/"):
            tile_folder_name = tile_folder_name[:-1]
        parsed = CHUNK_PATTERN.fullmatch(os.path.basename(tile_folder_name))
        tile = TileInfo(
            parsed.group("prefix"),
            int(parsed.group("run")),
            int(parsed.group("y")),
            int(parsed.group("z")),
            parsed.group("suffix"),
            tile_folder_name,
            SpoolSetInterpreter(tile_folder_name)
        )
        all_tiles_info.append(tile)
        # check no duplication
        if (tile.y, tile.z) in tiles_info_by_index:
            raise ValueError(
                f"Duplicate tile, file {tile.filename} conflicts with {tiles_info_by_index[(tile.y, tile.z)].filename}")
        tiles_info_by_index[(tile.y, tile.z)] = tile

    # default output name
    if not out:
        out = all_tiles_info[0].prefix + all_tiles_info[0].suffix
        out += ".nii.zarr" if nii else ".ome.zarr"
    nii = nii or out.endswith(".nii.zarr")

    # parse all individual file names
    z_tiles = set(tile.z for tile in all_tiles_info)
    y_tiles = set(tile.y for tile in all_tiles_info)
    min_y_tile, max_y_tile = min(y_tiles), max(y_tiles)
    min_z_tile, max_z_tile = min(z_tiles), max(z_tiles)
    num_y_tiles, num_z_tiles = len(y_tiles), len(z_tiles)

    all_shapes = np.empty((num_y_tiles, num_z_tiles, 3), dtype=int)

    shapes_x = defaultdict(list)
    dtypes = defaultdict(list)
    shapes_y_by_z = defaultdict(lambda: defaultdict(list))
    shapes_z_by_y = defaultdict(lambda: defaultdict(list))

    expected_sx = 0
    expected_sy = {}
    expected_sz = {}
    for z_tile in range(min_z_tile, max_z_tile + 1):
        for y_tile in range(min_y_tile, max_y_tile + 1):
            rel_y_tile_idx = y_tile - min_y_tile
            rel_z_tile_idx = z_tile - min_z_tile

            tile = tiles_info_by_index.get((y_tile, z_tile))
            if tile is None:
                warnings.warn(f"Missing tile {y_tile}, {z_tile}")
                continue
            reader = tile.reader
            # tile array is x, z, y
            sx, sz, sy = reader.spool_shape
            sx *= len(list(reader._get_spool_names_in_order()))
            dt = reader.dtype
            # Collect shapes and dtypes.
            all_shapes[rel_y_tile_idx, rel_z_tile_idx] = sx, sy, sz
            expected_sx = sx
            expected_sy[y_tile] = sy
            expected_sz[z_tile] = sz
            dtypes[dt].append((y_tile, z_tile))

            # Collect Y shapes per z_tile.
            shapes_y_by_z[z_tile][sy].append((y_tile, z_tile))
            # Collect Z shapes per y_tile.
            shapes_z_by_y[y_tile][sz].append((y_tile, z_tile))

    if len(dtypes) != 1:
        warnings.warn("Two or more dtypes in tiles:\n" + str(dict(dtypes)))

    diff_sx = (all_shapes[:, :, 0] != expected_sx)
    if diff_sx.any():
        y_idxs, z_idxs = np.where(diff_sx)
        raise ValueError(
            f"Inconsistent x shapes at indices: {list(zip(y_idxs, z_idxs))}")
    for y_tile in range(min_y_tile, max_y_tile + 1):
        if y_tile not in expected_sy:
            raise ValueError(f"Missing y tile {y_tile}")
        diff_sy = (all_shapes[:, :, 1] != expected_sy[y_tile])
        if diff_sy.any():
            y_idxs, z_idxs = np.where(diff_sy)
            raise ValueError(
                f"Inconsistent y shapes at tiles: {list(zip(y_idxs, z_idxs))}")
    for z_tile in range(min_z_tile, max_z_tile + 1):
        if z_tile not in expected_sz:
            raise ValueError(f"Missing z tile {z_tile}")
        diff_sz = (all_shapes[:, :, 2] != expected_sz[z_tile])
        if diff_sy.any():
            y_idxs, z_idxs = np.where(diff_sz)
            raise ValueError(
                f"Inconsistent z shapes at tiles: {list(zip(y_idxs, z_idxs))}")

    # Calculate full shapes using the assumption that the first tile in each direction represents the size.
    # Note: This assumes that a tile at index (y_tile, 1) or (1, z_tile) exists.
    full_shape_x = next(iter(shapes_x))
    full_shape_y = sum(expected_sy.values()) - (num_y_tiles - 1) * overlap
    full_shape_z = sum(expected_sz.values())
    fullshape = [full_shape_z, full_shape_y, full_shape_x]

    dtype = next(iter(dtypes))

    # Prepare Zarr group
    omz = zarr.storage.DirectoryStore(out)
    omz = zarr.group(store=omz, overwrite=True)
    print(out)
    # Prepare chunking options
    opt = {
        "chunks": [chunk] * 3,
        "dimension_separator": r"/",
        "order": "F",
        "dtype": np.dtype(dtype).str,
        "fill_value": None,
        "compressor": make_compressor(compressor, **compressor_opt),
    }

    # write first level
    omz.create_dataset("0", shape=fullshape, **opt)
    array = omz["0"]
    print("Write level 0 with shape", fullshape)

    for i, tile_info in enumerate(all_tiles_info):
        rel_y_tile_idx = tile_info.y - min_y_tile
        rel_z_tile_idx = tile_info.z - min_z_tile
        dat = tile_info.reader.assemble()

        if num_y_tiles != 1 and overlap != 0:
            # if not first y tile, crop half overlapped rows at the beginning
            if tile_info.y != min_y_tile:
                dat = dat[:, overlap // 2:, :]
            # if not last y tile, crop half overlapped rows at the end
            # if overlap is odd, we need to crop an extra row
            if tile_info.z != max_z_tile:
                dat = dat[:, :-overlap // 2 - (overlap % 2), :]
        ystart = sum(expected_sy[min_y_tile + y_idx] - overlap for y_idx in
                     range(rel_y_tile_idx))
        zstart = sum(expected_sz[min_z_tile + z_idx] for z_idx in range(rel_z_tile_idx))

        if rel_y_tile_idx != 0:
            ystart += overlap // 2
        print(
            f"Write plane "
            f"( {zstart} :{zstart + dat.shape[0]}, {ystart}:{ystart + dat.shape[1]})",
            end="\r",
        )
        slicer = (
            slice(zstart, zstart + dat.shape[0]),
            slice(ystart, ystart + dat.shape[1]),
            slice(None),
        )
        array[slicer] = dat
    print("")

    # build pyramid using median windows
    generate_pyramid(omz)

    write_ome_metadata(omz, axes = ["z","y","x"],space_scale=voxel_size)

    # Write NIfTI-Zarr header:
    if not nii:
        return

    header = default_nifti_header(omz["0"], omz)
    shape = list(reversed(omz["0"].shape))
    shape = shape[:3] + [1] + shape[3:]  # insert time dimension
    affine = orientation_to_affine(orientation, *voxel_size)
    if center:
        affine = center_affine(affine, shape[:3])
    header.set_data_shape(shape)
    header.set_data_dtype(omz["0"].dtype)
    header.set_qform(affine)
    header.set_sform(affine)
    header.set_xyzt_units(nib.nifti1.unit_codes.code["micron"])

    write_nifti_header(omz, header)

def read_spool_files(scan_path: str, scan_name: str, mat_path: str,
                     no_load: bool = False) -> np.ndarray:
    """
    Load SCAPE data from spool files for a given scan.

    Parameters:
        scan_path (str): The base path of the scan.
        scan_name (str): The name of the scan folder.
        mat_path (str): Path to the info .mat file.

    Returns:
        np.ndarray: The loaded SCAPE data reshaped into (num_columns, num_depths, total_frames).
    """
    # LOAD THE INFO FILE
    print(mat_path)
    if not os.path.exists(mat_path):
        warnings.warn('ERROR: Info file not found.')
        return None
    loaded_info = scipy.io.loadmat(mat_path)
    info = loaded_info["info"]

    # LOAD ZYLA METADATA
    zyla_info_path = os.path.join(scan_path, scan_name, 'acquisitionmetadata.ini')
    if not os.path.exists(zyla_info_path):
        warnings.warn('ERROR: No Zyla metadata found for this run.')
        return None

    config = ConfigParser()
    config.read(zyla_info_path, encoding='utf-8-sig')
    try:
        zyla = {
            'num_depths': int(config['data']['AOIHeight']),
            'num_columns': int(config['data']['AOIWidth']),
            'num_lat_pix': int(round(int(config['data']['AOIStride']) / 2)),
            'bytes_per_row': int(round(float(config['data']['AOIStride']))),
            'image_bytes': int(config['data']['ImageSizeBytes']),
            'num_frames_per_spool': int(config['multiimage']['ImagesPerFile']),
            'pixel_encoding': config['data']['PixelEncoding']
        }
    except KeyError as e:
        warnings.warn(f"Metadata key error: {e}")
        return None

    # Set bytes per pixel based on pixel encoding
    if zyla['pixel_encoding'] == 'Mono16':
        zyla['bytes_per_pixel'] = 2
        dtype = np.uint16
    elif zyla['pixel_encoding'] == 'Mono32':
        zyla['bytes_per_pixel'] = 4
        dtype = np.uint32
    else:
        warnings.warn('Issue with camera byte-precision (unexpected pixel encoding)!')
        dtype = np.uint16  # Fallback

    # NUMBER OF FRAMES
    camera_info = info['camera'][0][0]
    num_total_frames = int(camera_info['kineticSeriesLength'])
    num_bg_frames = int(camera_info['backgroundFramesNum'])
    num_frames_data = num_total_frames - num_bg_frames  # frames excluding BG
    num_frames_to_load = num_frames_data  # can be adjusted if needed

    # DETERMINE NUMBER OF SPOOL FILES
    num_frames_per_spool = zyla['num_frames_per_spool']
    num_columns = zyla['num_columns']
    num_depths = zyla['num_depths']

    num_spool_files_per_scan = math.ceil(num_frames_data / num_frames_per_spool)
    spool_folder = os.path.join(scan_path, scan_name)
    num_spool_files_actual = len(os.listdir(spool_folder))
    num_spool_files_to_load = math.ceil(num_frames_to_load / num_frames_per_spool)

    print(f"Expected spool files: {num_spool_files_per_scan}")
    print(f"Actual spool files: {num_spool_files_actual}")
    print(f"SPOOL files to load: {num_spool_files_to_load}")

    if num_spool_files_per_scan > num_spool_files_actual:
        print(
            f"{scan_name} : Not enough spool files - possible failed run, loading max available")
        num_spool_files_to_load = num_spool_files_actual

    # GENERATE LIST OF SPOOL FILE NAMES
    spool_files_total = [f"{i:010d}"[::-1] + "spool.dat" for i in
                         range(num_spool_files_actual + 1)]
    spool_files_to_load = spool_files_total[:num_spool_files_to_load]

    # HANDLE IMAGE SIZE
    bin_factor = int(camera_info['binFactor'])
    if bin_factor == 2:
        num_rows_read = num_depths + 1  # handle binning
    else:
        num_rows_read = num_depths + (1 if num_depths % 2 == 1 else 2)
    num_columns_read = zyla['bytes_per_row'] // zyla['bytes_per_pixel']

    # PRE-ALLOCATE MEMORY FOR SPOOLS
    scape_data = np.zeros(
        (num_columns, num_depths, num_frames_per_spool, num_spool_files_to_load),
        dtype=dtype
    )
    if no_load:
        return scape_data.reshape(num_columns, num_depths, -1)
    num_pixels_to_reshape = num_rows_read * num_columns_read * num_frames_per_spool

    # PREPARE FOR THE DATA READ
    print(f"Zyla pixel encoding: {zyla['pixel_encoding']}")
    print(
        f"Bytes per row: {zyla['bytes_per_row']}, num_columns: {zyla['num_columns']}, "
        f"ratio: {zyla['bytes_per_row'] / zyla['num_columns']}")

    # READ DATA FROM EACH SPOOL FILE
    start_time = time.time()
    for spool_idx, spool_file in enumerate(spool_files_to_load):
        raw_data_path = os.path.join(spool_folder, spool_file)
        if os.path.exists(raw_data_path):
            with open(raw_data_path, 'rb') as fid:
                raw_data = np.fromfile(fid, dtype=dtype)
            # Use only the necessary pixels and reshape to 3D block:
            raw_data = raw_data[:num_pixels_to_reshape].reshape(
                (num_columns_read, num_rows_read, num_frames_per_spool)
            )
            # Crop out buffer pixels
            raw_data = raw_data[:num_columns, :num_depths, :]
            scape_data[:, :, :, spool_idx] = raw_data.astype(dtype)
        else:
            warnings.warn(f"Spool file not found: {raw_data_path}")

    # Combine spool files by reshaping to (num_columns, num_depths, total_frames)
    total_frames = num_frames_per_spool * num_spool_files_to_load
    scape_data = scape_data.reshape(num_columns, num_depths, total_frames)
    elapsed_time = time.time() - start_time
    print(
        f"{scan_name}: Dataset loaded - dims: {scape_data.shape[0]} x {scape_data.shape[1]} x {scape_data.shape[2]}  (time: {elapsed_time:.2f} sec)")

    return scape_data


# from compression_tools.alt_zip import alt_zip
import configparser
import numpy as np
import io
import warnings
import os


class SpoolSetInterpreter:

    def __init__(self, compression_tools_zip_file):
        if os.path.isfile(compression_tools_zip_file) and \
                os.path.splitext(compression_tools_zip_file)[-1] == '.zip':
            self.type = '.zip'
            self.compression_tools_zip_file = compression_tools_zip_file
            self.location = compression_tools_zip_file
            self.spool_set = alt_zip(self.location)
        elif os.path.isdir(compression_tools_zip_file):
            self.type = 'dir'
            self.parent = compression_tools_zip_file
            self.file_list = glob(os.path.join(compression_tools_zip_file, '*'))
            self.spool_set = tuple([os.path.split(x)[-1] for x in self.file_list])
        else:
            assert False, 'The input data structure is not a ZIP file or Directory'

        self._what_spool_format()
        self.spool_files = tuple(self._get_spool_names_in_order())  # In order
        self._get_acquisitionparameters_str()
        self._get_config()
        self._extract_config_values()

    def _make_filename_from_spool_set(self, spool_entry):
        return os.path.join(self.parent, spool_entry)

    @property
    def entries(self):
        if self.type == '.zip':
            return self.spool_set.entries
        elif self.type == 'dir':
            return self.spool_set

    def _what_spool_format(self):
        if 'Spooled files.sifx' in self.entries:
            format = 'zyla'
        else:
            raise TypeError("Unknown or unsupported spool file format")

        self.format = format

    def _list_spool_files(self):
        return sorted(
            tuple(
                [x for x in self.entries if '0spool.dat' in x]
            )
        )

    def _get_acquisitionparameters_str(self):
        if self.type == '.zip':
            ini = self.spool_set['acquisitionmetadata.ini']
        elif self.type == 'dir':
            file = self._make_filename_from_spool_set('acquisitionmetadata.ini')
            with open(file, 'rb') as f:
                ini = f.read()
        ini = ini.decode('UTF-8-sig')  # Encoding for acquisitionmetadata.ini
        self.acquisitionparameters_str = ini

    def _get_config(self):
        buf = io.StringIO(self.acquisitionparameters_str)
        self.config = configparser.ConfigParser()
        self.config.read_file(buf)

    def _extract_config_values(self):
        self.acquisition_metadata = {}
        # ini info
        self.acquisition_metadata['height'] = self.config.getint('data', 'AOIHeight')
        self.acquisition_metadata['width'] = self.config.getint('data', 'AOIWidth')
        self.acquisition_metadata['stride'] = self.config.getint('data', 'AOIStride')
        dtype = self.config.get('data', 'PixelEncoding')

        if dtype == 'Mono16':
            dtype = np.dtype('uint16')
        elif dtype == 'Mono8':
            dtype = np.dtype('uint8')

        self.acquisition_metadata['dtype'] = dtype
        self.dtype = dtype

        self.spool_nbytes = self.config.getint('data', 'ImageSizeBytes')
        self.acquisition_metadata['nbytes'] = self.spool_nbytes

        self.acquisition_metadata['images'] = self.config.getint('multiimage',
                                                                 'ImagesPerFile')

        numDepths = self.acquisition_metadata['height']
        numLatPix = self.acquisition_metadata['stride'] // 2
        imageBytes = self.acquisition_metadata['nbytes']
        numFramesPerSpool = self.acquisition_metadata['images']
        startIndex = self.acquisition_metadata['nbytes']
        imageSize = self.acquisition_metadata['nbytes']

        numRows = numDepths + 2

        if numDepths % 2:  # if there is an odd number of rows ->  KPEDIT - odd rows means 1 less column for some reason
            numRows = numDepths + 1

        numColumns = numLatPix

        self.spool_shape = (numFramesPerSpool, numRows, numColumns)

    def _load_spool_file(self, spool_file_name):
        if self.type == '.zip':
            array = np.frombuffer(self.spool_set[spool_file_name], dtype=self.dtype)
        elif self.type == 'dir':
            file = self._make_filename_from_spool_set(spool_file_name)
            print(f'Reading file {spool_file_name}')
            with open(file, 'rb') as f:
                array = np.frombuffer(f.read(), dtype=self.dtype)
        return np.reshape(array, self.spool_shape)

    def __getitem__(self, key):
        if isinstance(key, str):
            assert key in self.spool_files, 'Must be a spool file in self.spool_files OR integer index in self.spool_files'
            return self._load_spool_file(key)
        elif isinstance(key, int):
            return self._load_spool_file(self.spool_files[key])

    def __iter__(self):
        yield from (self[x] for x in range(len(self)))

    def __contains__(self, item):
        return item in self.spool_files

    def __len__(self):
        return len(self.spool_files)

    def _get_spool_names_in_order(self):
        '''
        Spool files are ordered sequentially in the order they were collected 0,1,2,...,201,202,203,...
        but file names are recorded as the reversed number padded to
        10 digits (0000000000,1000000000,20000000000,...,1020000000,2020000000,3020000000,...) + spool.dat
        '''
        spool_files = self._list_spool_files()
        misses = 0
        for idx in range(len(spool_files)):
            # Convert index to string, pad with zeros to 10 digits and reverse
            tmp = str(idx + misses).zfill(10)[::-1]
            tmp = f'{tmp}spool.dat'
            if tmp in spool_files:
                yield tmp
            else:
                warnings.warn(f"{tmp} not located in spool directory")
                misses += 1

    def assemble(self):
        axis_0_shape = self.spool_shape[0]
        canvas = np.zeros((axis_0_shape * len(self), *self.spool_shape[1:]),
                          dtype=self.dtype)
        for idx, spool_file in enumerate(self):
            start = idx * axis_0_shape
            stop = start + axis_0_shape
            canvas[start:stop] = spool_file
        print(canvas.shape)
        return canvas.transpose(1, 2, 0)
