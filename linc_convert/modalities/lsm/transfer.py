"""
Convert a collection of spool .dat files generated by light sheet microscopy to compressed tar files and upload with the DANDI client.
"""

# stdlib
import os
from datetime import datetime
from pathlib import Path

# externals
import tarfile
import cyclopts
import dandi.download
import dandi.upload
from dandi.dandiapi import DandiAPIClient

# internals
from linc_convert.modalities.lsm.cli import lsm

transfer = cyclopts.App(name="transfer", help_format="markdown")
lsm.command(transfer)

@transfer.default
def dandi_transfer(input_dir, dandiset_url, dandi_instance, subject, output_dir='.',  max_size_gb=2.00, upload=True):
    """
    Upload .dat files to DANDI in batched, compressed tar archives.
    
    Parameters
    ----------
    input_dir : str
        Directory containing .dat files to upload
    dandiset_url : str
        URL for the dandiset to upload (e.g., https://lincbrain.org/dandiset/000010)
    dandi_instance : str
        DANDI server (e.g. linc, dandi)
    output_dir : str, optional
        Directory to save the Dandiset directory (default: '.')
    max_size_gb : float, optional
        Maximum size for each archive in GB (default: 2)
    upload : bool, optional
        Whether to upload the tar files to DANDI (default: True)
    """

    max_size_bytes = int(max_size_gb * 1024 * 1024 * 1024)


    client = DandiAPIClient("https://api.lincbrain.org/api")
    client.dandi_authenticate()
    dandi.download.download(dandiset_url, output_dir=output_dir)

    dandiset_id = dandiset_url.split('/')[-1]
    dandiset_directory = f'{output_dir}/{dandiset_id}'

    if not os.path.exists(f'{dandiset_directory}/dataset_description.json'):
        with open(f'{dandiset_directory}/dataset_description.json', 'w') as f:
            f.write('{}')

    archive_directory = f'{dandiset_directory}/sourcedata/sub-{subject}'
    os.makedirs(archive_directory, exist_ok=True)

    dat_files = list(Path(input_dir).glob("*.dat"))
    dat_files_size = len(dat_files)
    if dat_files_size:
        print(f"Found {dat_files_size} .dat files in '{input_dir}'.")
    else:
        print(f"No .dat files found in '{input_dir}'.")
        return

    batch = 0
    file_number = 0

    while file_number < dat_files_size:

        print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Creating archive batch {batch}")

        archive_path = os.path.join(archive_directory, f"sub-{subject}_desc-batch{batch}.tar")
        archive = tarfile.open(archive_path, "w")
        
        batch_size = 0
        batch_files = 0

        while batch_size < max_size_bytes and file_number < dat_files_size:

            file_path = dat_files[file_number]
            file_size = os.path.getsize(file_path)
            
            print(f"Adding '{file_path.name}' ({file_size/1024**2:.2f}MB, {file_number}) to archive.")
            archive.add(file_path, arcname=file_path.name)

            batch_size += file_size
            batch_files += 1
            file_number += 1

        archive.close()
    
        print(f"Archive created with {batch_files} files and {batch_size / 1024**2:.2f}MB size.")
        
        if upload:
            print(f"Uploading {archive_path}.")
            dandi.upload.upload([dandiset_directory],
                                dandi_instance=dandi_instance,
                                )
            os.remove(archive_path)

        del archive
        os.remove(archive_path)
        batch += 1

        print(f"Progress: {file_number}/{dat_files_size} files processed ({file_number/dat_files_size*100:.2f}%).")
    
    print(f"{file_number} files uploaded successfully.")
