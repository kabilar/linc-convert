{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the LINC Convert Documentation","text":"<p>The center for Large-scale Imaging of Neural Circuits (LINC)  (PIs: Haber, Hillman, Yendiki) is funded by the  NIH BRAIN Initiative CONNECTS program. Its goal is to develop novel technologies for imaging brain connections down to  the microscopic scale, and deploy these technologies to image  cortico-subcortical projections that are relevant to deep brain stimulation for  motor and psychiatric disorders.</p>"},{"location":"#about-this-doc","title":"About this doc","text":"<p>The <code>linc-convert</code> package converts dark-field microscopy, light-sheet microscopy, and polarization sensitive optical coherence tomography files to the OME-Zarr file format.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>LINC Homepage</li> <li>LINC data conversion code on GitHub</li> </ul>"},{"location":"#support","title":"Support","text":"<p>For questions, bug reports, and feature requests, please file an issue on the linc-convert repository.</p>"},{"location":"about/","title":"About this doc","text":""},{"location":"about/#acknowledgements","title":"Acknowledgements","text":"<p>Thank you to the DANDI Archive project for setting up the documentation framework that is utilized here.  See the DANDI Handbook for more information.</p>"},{"location":"about/#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"contribute/","title":"Contribute to this documentation","text":"<p>If you find an issue with this documentation please file an issue or submit a pull request on the linc-convert repository.</p> <p>If you would like to contribute to the LINC documentation and render the documentation locally as you make edits, please follow the steps below:</p> <ol> <li>Fork the linc-convert repository and clone it to your computer.</li> <li>Set up a Python environment with the dependencies in the requirements.txt file.</li> <li>Within the Python environment, run <code>mkdocs serve</code>.  This will build the website and start a local webserver (e.g. at http://127.0.0.1:8000) with your documentation.</li> <li>As you continue to edit the markdown files or configuration file, your documentation will be automatically re-built and rendered locally.</li> <li>Commit your changes and submit a pull request.</li> </ol>"},{"location":"linc-convert/","title":"API","text":"<p>Data conversion tools for the LINC project.</p>"},{"location":"start/","title":"Getting started","text":"<p>All data stored on the LINC Data Platform is private to the LINC team. So it should not be made public without written approval from the dataset owner and Anastasia.</p>"},{"location":"upload/","title":"Uploading data to lincbrain.org","text":"<p>Note: The steps below require access to https://lincbrain.org, which is a private repository for LINC project investigators.</p>"},{"location":"upload/#create-a-new-dataset-or-contribute-to-an-existing-dataset","title":"Create a new dataset or contribute to an existing dataset","text":"<p>A dataset refers to a collection of brains that have been processed and imaged in a similar way. It typically contains data from multiple brains or samples, possibly imaged with multiple modalities.</p> <ol> <li>Create a new dataset<ol> <li>Log into the LINC Data Platform.</li> <li>Click on the <code>NEW DATASET</code> button at the top right of the page.</li> <li>Fill out the title, description, and license. (The license option exists only because lincbrain.org is a clone of DANDI. It has no effect here, as lincbrain.org is a private repository.)</li> <li>Click on the <code>REGISTER DATASET</code> button to create the new dataset.</li> </ol> </li> <li>Contribute to an existing dataset<ol> <li>Log into the LINC Data Platform.</li> <li>Browse the existing datasets under the SHARED DATASETS tab.</li> <li>Contact the <code>Owner</code> by email requesting that they add you as an <code>Owner</code> of the dataset.</li> </ol> </li> </ol>"},{"location":"upload/#install-the-dandi-command-line-interface-cli","title":"Install the dandi command-line interface (CLI)","text":"<p>On your local machine, install the dandi CLI package in a python environment:</p> <p><code>pip install dandi</code></p>"},{"location":"upload/#copy-your-lincbrain-api-key","title":"Copy your lincbrain API key","text":"<p>Log into lincbrain.org and click on the button with your initials at the top right of the page. Copy your API key and enter it in the following environment variable on your local machine:</p> <p><code>export DANDI_API_KEY=&lt;EnterYourKeyHere&gt;</code></p>"},{"location":"upload/#download-your-new-empty-dataset-locally","title":"Download your new (empty) dataset locally","text":"<p>You can find the command that you need to run to download a specific dataset by navigating to the dataset landing page on lincbrain.org, clicking on the <code>DOWNLOAD</code> drop-down menu that you'll see at the top right corner of that page, and copying the <code>dandi download ...</code> command that you see when you click on that menu. </p> <p>On your local machine, create a directory that you will use as a staging area for uploading data. Then cd into this directory, and run the download command that you copied above. For example:</p> <pre><code>cd /path/to/my/staging/area\ndandi download https://lincbrain.org/dandiset/101010/draft\n</code></pre> <p>The above example will create a directory called <code>/path/to/my/staging/area/101010</code> with a file called <code>dandiset.yaml</code> in it. Any data files that you want to upload to your new lincbrain.org dataset have to first be saved here, and organized according to the Brain Imaging Data Structure (BIDS).</p>"},{"location":"upload/#organize-your-data","title":"Organize your data","text":"<p>An example of how to organize a dataset that includes dMRI and histology data from two brains is shown below:</p> <pre><code>101010/\n  dataset_description.json\n  participants.tsv\n  rawdata/\n    sub-Ken1/\n      dwi/\n        sub-Ken1_acq-DSI_dwi.bval\n        sub-Ken1_acq-DSI_dwi.bvec\n        sub-Ken1_acq-DSI_dwi.json\n        sub-Ken1_acq-DSI_dwi.nii.gz\n      micr/\n        samples.tsv\n        sub-Ken1_sample-slice0001_photo.json\n        sub-Ken1_sample-slice0001_photo.tif\n        sub-Ken1_sample-slice0001_stain-Nissl_BF.json\n        sub-Ken1_sample-slice0001_stain-Nissl_BF.tif\n        sub-Ken1_sample-slice0002_photo.json\n        sub-Ken1_sample-slice0002_photo.tif\n        sub-Ken1_sample-slice0002_stain-LY_DF.json\n        sub-Ken1_sample-slice0002_stain-LY_DF.tif\n        sub-Ken1_sample-slice0009_photo.json\n        sub-Ken1_sample-slice0009_photo.tif\n        sub-Ken1_sample-slice0009_stain-Nissl_BF.json\n        sub-Ken1_sample-slice0009_stain-Nissl_BF.tif\n        sub-Ken1_sample-slice0010_photo.json\n        sub-Ken1_sample-slice0010_photo.tif\n        sub-Ken1_sample-slice0010_stain-LY_DF.json\n        sub-Ken1_sample-slice0010_stain-LY_DF.tif\n    sub-Ken2/  \n      dwi/\n        sub-Ken2_acq-DSI_dwi.bval\n        sub-Ken2_acq-DSI_dwi.bvec\n        sub-Ken2_acq-DSI_dwi.json\n        sub-Ken2_acq-DSI_dwi.nii.gz\n        sub-Ken2_acq-MulShellMulTE_dwi.bval\n        sub-Ken2_acq-MulShellMulTE_dwi.bvec\n        sub-Ken2_acq-MulShellMulTE_dwi.json\n        sub-Ken2_acq-MulShellMulTE_dwi.nii.gz\n      micr/\n        samples.tsv\n        sub-Ken2_sample-slice0001_photo.json\n        sub-Ken2_sample-slice0001_photo.tif\n        sub-Ken2_sample-slice0001_stain-Nissl_BF.json\n        sub-Ken2_sample-slice0001_stain-Nissl_BF.tif\n        sub-Ken2_sample-slice0002_photo.json\n        sub-Ken2_sample-slice0002_photo.tif\n        sub-Ken2_sample-slice0002_stain-LY_DF.json\n        sub-Ken2_sample-slice0002_stain-LY_DF.tif\n        sub-Ken2_sample-slice0003_photo.json\n        sub-Ken2_sample-slice0003_photo.tif\n        sub-Ken2_sample-slice0003_stain-FR_DF.json\n        sub-Ken2_sample-slice0003_stain-FR_DF.tif\n        sub-Ken2_sample-slice0009_photo.json\n        sub-Ken2_sample-slice0009_photo.tif\n        sub-Ken2_sample-slice0009_stain-Nissl_BF.json\n        sub-Ken2_sample-slice0009_stain-Nissl_BF.tif\n        sub-Ken2_sample-slice0010_photo.json\n        sub-Ken2_sample-slice0010_photo.tif\n        sub-Ken2_sample-slice0010_stain-LY_DF.json\n        sub-Ken2_sample-slice0010_stain-LY_DF.tif\n        sub-Ken2_sample-slice0011_photo.json\n        sub-Ken2_sample-slice0011_photo.tif\n        sub-Ken2_sample-slice0011_stain-FR_DF.json\n        sub-Ken2_sample-slice0011_stain-FR_DF.tif\n</code></pre> <p>The files and subdirectories in this example dataset are described in detail below.</p>"},{"location":"upload/#dataset_descriptionjson","title":"dataset_description.json","text":"<p>This text file is described in detail in the BIDS specification. A minimal <code>dataset_description.json</code> would look like this:</p> <pre><code>{\n    \"Name\": \"Seminal post mortem dMRI and histology dataset from the laboratory of Dr. Barbara Millicent Roberts\",\n    \"BIDSVersion\": \"1.9.0\"\n}\n</code></pre>"},{"location":"upload/#participantstsv","title":"participants.tsv","text":"<p>This text file is described in detail in the BIDS specification. For this dataset, the <code>participants.tsv</code> might look like this:</p> <pre><code>participant_id age sex diagnosis\nsub-Ken1 43 M healthy\nsub-Ken2 61 M hypertension\n</code></pre>"},{"location":"upload/#rawdata","title":"rawdata","text":"<p>This directory contains one subdirectory for each brain, which contain one subdirectory for each modality, which in turn contain raw image data files named according to the BIDS specification.</p>"},{"location":"upload/#dwi","title":"dwi","text":"<p>This directory contains dMRI data files as described in detail in the BIDS specification. </p> <p>In this example the data include images (<code>.nii.gz</code>), b-value tables (<code>.bval</code>), gradient tables (<code>.bvec</code>), and metadata (<code>.json</code>). Data from Ken1 were acquired with a DSI scheme, whereas data from Ken2 were acquired both with a DSI scheme and with a multi-shell, multi-echo scheme.</p>"},{"location":"upload/#micr","title":"micr","text":"<p>This directory contains microscopy data files as described in detail in the BIDS specification.</p> <p>In this example the data include images (<code>.tif</code>) and metadata (<code>.json</code>) from multiple brain sections. For each section there is a blockface photo (<code>_photo</code>) and a histological stain (<code>_stain</code>). Sections from Ken1 and Ken2 were either processed with a Nissl stain and imaged under brightfield microscopy (<code>_BF</code>), or processed for the fluorescent tracer Lucifer Yellow (<code>LY</code>) and imaged under darkfield microscopy (<code>_DF</code>). Additional sections from Ken2 were processed for the fluorescent tracer Fluoro-Ruby (<code>FR</code>) and imaged under darkfield microscopy (<code>_DF</code>).</p>"},{"location":"upload/#high-res-histology-annotation","title":"high-res histology annotation","text":"<p>All annotation files use the following naming scheme: <code>&lt;dataset-name&gt; + _desc-[label] + _suffix.ome.zarr</code> where <code>[label]</code> is replaced by the annotator's initials and <code>_suffix</code> indicates the type of segmentations being annotated. Specifically, when annotating discrete segmentations, use <code>_dseg</code> as the suffix (see the BIDS spec on discrete segmentations).</p> <p>For example, an annotator with an initial JS annotating discrete segments would name the annotation file as <code>000003_sub-MR243_sample-slice0000slice0004_stain-LY_DF_desc-JS_dseg.ome.zarr</code></p> <p>A <code>&lt;matches&gt;.tsv</code> file could be included to map the IDs (integer values) of the discrete segmentations to the custom labels, where <code>&lt;matches&gt;</code> is replaced by the name of the annotation file. It contains a lookup table with the following columns (see the BIDS spec on custom TSV):</p> <pre><code>index  name\n1      Single Fiber\n2      Light Bundle\n3      Moderate Bundle\n...\n</code></pre>"},{"location":"upload/#samplestsv","title":"samples.tsv","text":"<p>This text file is described in detail in the BIDS specification. For Ken1, the <code>samples.tsv</code> would look like this:</p> <pre><code>participant_id sample_id sample_type\nsub-Ken1 sample-slice0001 tissue\nsub-Ken1 sample-slice0002 tissue\nsub-Ken1 sample-slice0009 tissue\nsub-Ken1 sample-slice0010 tissue\n</code></pre>"},{"location":"upload/#upload-your-data","title":"Upload your data","text":"<p>Upload the data from your local machine to lincbrain.org:</p> <pre><code>cd /path/to/my/staging/area/101010\ndandi upload -i linc\n</code></pre> <p>Check the output in your terminal for validation errors. If there were no errors, your data files should now appear on lincbrain.org.</p>"},{"location":"upload/#delete-data-files-or-directories","title":"Delete data files or directories","text":"<p>Individual data files can be deleted on lincbrain.org by clicking on the trashcan icon next to each file. Alternatively, directories or files can be deleted from the command line.</p> <p>The following examples delete a directory named \"horses\" on lincbrain.org:</p> <pre><code>dandi delete -i linc /path/to/my/staging/area/101010/rawdata/Ken2/horses\n</code></pre> <pre><code>dandi delete -i linc \"https://lincbrain.org/dandiset/101010/draft/files?location=rawdata%2Fsub-Ken2%2Fhorses\"\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cli</li> <li>modalities<ul> <li>df<ul> <li>cli</li> <li>multi_slice</li> <li>single_slice</li> </ul> </li> <li>lsm<ul> <li>cli</li> <li>mosaic</li> </ul> </li> </ul> </li> <li>utils<ul> <li>j2k</li> <li>math</li> <li>orientation</li> <li>zarr</li> </ul> </li> </ul>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/cli/","title":"Cli","text":"<p>Root command line entry point.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/__init__/","title":"init","text":"<p>Converters for all imaging modalities.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/__init__/","title":"init","text":"<p>Dark Field microscopy converters.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/cli/","title":"Cli","text":"<p>Entry-points for Dark Field microscopy converter.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/multi_slice/","title":"Multi slice","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a OME-ZARR pyramid.</p> <p>We do not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/multi_slice/#linc_convert.modalities.df.multi_slice.convert","title":"<code>convert(inp, out=None, *, chunk=4096, compressor='blosc', compressor_opt='{}', max_load=16384, nii=False, orientation='coronal', center=True, thickness=None)</code>","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p> <p>This command converts a batch of slices and stacks them together into a single 3D Zarr.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/multi_slice/#linc_convert.modalities.df.multi_slice.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and   indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and   indicates the anatomical meaning of the bottom of the jp2 image,</li> <li>the third letter corresponds to the slice dimension and   indicates the anatomical meaninff of the end of the stack.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/multi_slice/#linc_convert.modalities.df.multi_slice.convert--parameters","title":"Parameters","text":"<p>inp     Path to the input slices out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\" orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center thickness     Slice thickness Source code in <code>linc_convert/modalities/df/multi_slice.py</code> <pre><code>@ms.default\ndef convert(\n    inp: list[str],\n    out: str | None = None,\n    *,\n    chunk: int = 4096,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 16384,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.\n\n    It does not recompute the image pyramid but instead reuse the\n    JPEG2000 levels (obtained by wavelet transform).\n\n    This command converts a batch of slices and stacks them together\n    into a single 3D Zarr.\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n      indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n      indicates the anatomical meaning of the _bottom_ of the jp2 image,\n    * the third letter corresponds to the slice dimension and\n      indicates the anatomical meaninff of the _end_ of the stack.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the input slices\n    out\n        Path to the output Zarr directory [&lt;dirname(INP)&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\"\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    thickness\n        Slice thickness\n    \"\"\"\n    # Default output path\n    if not out:\n        out = os.path.splitext(inp[0])[0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    nblevel, has_channel, dtype_jp2 = float(\"inf\"), float(\"inf\"), \"\"\n\n    # Compute output shape\n    new_height, new_width = 0, 0\n    for inp1 in inp:\n        jp2 = glymur.Jp2k(inp1)\n        nblevel = min(nblevel, jp2.codestream.segment[2].num_res)\n        has_channel = min(has_channel, jp2.ndim - 2)\n        dtype_jp2 = np.dtype(jp2.dtype).str\n        if jp2.shape[0] &gt; new_height:\n            new_height = jp2.shape[0]\n        if jp2.shape[1] &gt; new_width:\n            new_width = jp2.shape[1]\n    new_size = (new_height, new_width)\n    if has_channel:\n        new_size += (3.0,)\n    print(len(inp), new_size, nblevel, has_channel)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": list(new_size[2:]) + [1] + [chunk, chunk],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": dtype_jp2,\n        \"fill_value\": 0,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n    print(opt)\n    print(new_size)\n    # Write each level\n    for level in range(nblevel):\n        shape = [ceildiv(s, 2**level) for s in new_size[:2]]\n        shape = [new_size[2]] + [len(inp)] + shape\n\n        omz.create_dataset(f\"{level}\", shape=shape, **opt)\n        array = omz[f\"{level}\"]\n\n        # Write each slice\n        for idx, inp1 in enumerate(inp):\n            j2k = glymur.Jp2k(inp1)\n            vxw, vxh = get_pixelsize(j2k)\n            subdat = WrappedJ2K(j2k, level=level)\n            subdat_size = subdat.shape\n            print(\n                \"Convert level\",\n                level,\n                \"with shape\",\n                shape,\n                \"for slice\",\n                idx,\n                \"with size\",\n                subdat_size,\n            )\n\n            # offset while attaching\n            x = floordiv(shape[-2] - subdat_size[-2], 2)\n            y = floordiv(shape[-1] - subdat_size[-1], 2)\n\n            if max_load is None or (shape[-2] &lt; max_load and shape[-1] &lt; max_load):\n                array[..., idx, x : x + subdat_size[1], y : y + subdat_size[2]] = (\n                    subdat[...]\n                )\n\n            else:\n                ni = ceildiv(shape[-2], max_load)\n                nj = ceildiv(shape[-1], max_load)\n\n                for i in range(ni):\n                    for j in range(nj):\n                        print(f\"\\r{i+1}/{ni}, {j+1}/{nj}\", end=\" \")\n                        start_x, end_x = (\n                            i * max_load,\n                            min((i + 1) * max_load, shape[-2]),\n                        )\n                        start_y, end_y = (\n                            j * max_load,\n                            min((j + 1) * max_load, shape[-1]),\n                        )\n                        if end_x &lt;= x or end_y &lt;= y:\n                            continue\n\n                        if start_x &gt;= subdat_size[-2] or start_y &gt;= subdat_size[-1]:\n                            continue\n\n                        array[\n                            ...,\n                            idx,\n                            x + start_x : x + min(end_x, subdat_size[-2]),\n                            y + start_y : y + min(end_y, subdat_size[-1]),\n                        ] = subdat[\n                            ...,\n                            start_x : min((i + 1) * max_load, subdat_size[-2]),\n                            start_y : min((j + 1) * max_load, subdat_size[-1]),\n                        ]\n                print(\"\")\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"z\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"y\", \"type\": \"distance\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"jpeg2000\",\n            \"name\": \"\",\n        }\n    ]\n    if has_channel:\n        multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    for n in range(nblevel):\n        shape0 = omz[\"0\"].shape[-2:]\n        shape = omz[str(n)].shape[-2:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # I assume that wavelet transforms end up aligning voxel edges\n        # across levels, so the effective scaling is the shape ratio,\n        # and there is a half voxel shift wrt to the \"center of first voxel\"\n        # frame\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0] * has_channel\n                + [\n                    1.0,\n                    (shape0[0] / shape[0]) * vxh,\n                    (shape0[1] / shape[1]) * vxw,\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0] * has_channel\n                + [\n                    0.0,\n                    (shape0[0] / shape[0] - 1) * vxh * 0.5,\n                    (shape0[1] / shape[1] - 1) * vxw * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * (3 + has_channel), \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    if has_channel:\n        shape = shape[:3] + [1] + shape[3:]\n    affine = orientation_to_affine(orientation, vxw, vxh, thickness or 1)\n    if center:\n        affine = center_affine(affine, shape[:2])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"n+2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n\n    # Write sidecar .json file\n    json_name = os.path.splitext(out)[0]\n    json_name += \".json\"\n    dic = {}\n    dic[\"PixelSize\"] = json.dumps([vxw, vxh])\n    dic[\"PixelSizeUnits\"] = \"um\"\n    dic[\"SliceThickness\"] = 1.2\n    dic[\"SliceThicknessUnits\"] = \"mm\"\n    dic[\"SampleStaining\"] = \"LY\"\n\n    with open(json_name, \"w\") as outfile:\n        json.dump(dic, outfile)\n        outfile.write(\"\\n\")\n\n    print(\"done.\")\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/single_slice/","title":"Single slice","text":"<p>Converts JPEG2000 files generated by MBF-Neurolucida into a OME-ZARR pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/single_slice/#linc_convert.modalities.df.single_slice.convert","title":"<code>convert(inp, out=None, *, chunk=1024, compressor='blosc', compressor_opt='{}', max_load=16384, nii=False, orientation='coronal', center=True, thickness=None)</code>","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/single_slice/#linc_convert.modalities.df.single_slice.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and     indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and     indicates the anatomical meaning of the bottom of the jp2 image.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/df/single_slice/#linc_convert.modalities.df.single_slice.convert--parameters","title":"Parameters","text":"<p>inp     Path to the input JP2 file out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\" orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center thickness     Slice thickness Source code in <code>linc_convert/modalities/df/single_slice.py</code> <pre><code>@ss.default\ndef convert(\n    inp: str,\n    out: str | None = None,\n    *,\n    chunk: int = 1024,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 16384,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.\n\n    It does not recompute the image pyramid but instead reuse the JPEG2000\n    levels (obtained by wavelet transform).\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n        indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n        indicates the anatomical meaning of the _bottom_ of the jp2 image.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the input JP2 file\n    out\n        Path to the output Zarr directory [&lt;INP&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\"\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    thickness\n        Slice thickness\n    \"\"\"\n    if not out:\n        out = os.path.splitext(inp)[0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    j2k = glymur.Jp2k(inp)\n    vxw, vxh = get_pixelsize(j2k)\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": list(j2k.shape[2:]) + [chunk, chunk],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": np.dtype(j2k.dtype).str,\n        \"fill_value\": None,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n\n    # Write each level\n    nblevel = j2k.codestream.segment[2].num_res\n    has_channel = j2k.ndim - 2\n    for level in range(nblevel):\n        subdat = WrappedJ2K(j2k, level=level)\n        shape = subdat.shape\n        print(\"Convert level\", level, \"with shape\", shape)\n        omz.create_dataset(str(level), shape=shape, **opt)\n        array = omz[str(level)]\n        if max_load is None or (shape[-2] &lt; max_load and shape[-1] &lt; max_load):\n            array[...] = subdat[...]\n        else:\n            ni = ceildiv(shape[-2], max_load)\n            nj = ceildiv(shape[-1], max_load)\n            for i in range(ni):\n                for j in range(nj):\n                    print(f\"\\r{i+1}/{ni}, {j+1}/{nj}\", end=\"\")\n                    array[\n                        ...,\n                        i * max_load : min((i + 1) * max_load, shape[-2]),\n                        j * max_load : min((j + 1) * max_load, shape[-1]),\n                    ] = subdat[\n                        ...,\n                        i * max_load : min((i + 1) * max_load, shape[-2]),\n                        j * max_load : min((j + 1) * max_load, shape[-1]),\n                    ]\n            print(\"\")\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"y\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"jpeg2000\",\n            \"name\": \"\",\n        }\n    ]\n    if has_channel:\n        multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    for n in range(nblevel):\n        shape0 = omz[\"0\"].shape[-2:]\n        shape = omz[str(n)].shape[-2:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # I assume that wavelet transforms end up aligning voxel edges\n        # across levels, so the effective scaling is the shape ratio,\n        # and there is a half voxel shift wrt to the \"center of first voxel\"\n        # frame\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0] * has_channel\n                + [\n                    (shape0[0] / shape[0]) * vxh,\n                    (shape0[1] / shape[1]) * vxw,\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0] * has_channel\n                + [\n                    (shape0[0] / shape[0] - 1) * vxh * 0.5,\n                    (shape0[1] / shape[1] - 1) * vxw * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * (2 + has_channel), \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    if not nii:\n        print(\"done.\")\n        return\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    if has_channel:\n        shape = shape[:2] + [1, 1] + shape[2:]\n    affine = orientation_to_affine(orientation, vxw, vxh, thickness or 1)\n    if center:\n        affine = center_affine(affine, shape[:2])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"n+2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n    print(\"done.\")\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/lsm/__init__/","title":"init","text":"<p>Light Sheet Microscopy converters.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/lsm/cli/","title":"Cli","text":"<p>Entry-points for Dark Field microscopy converter.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/lsm/mosaic/","title":"Mosaic","text":"<p>Convert a collection of tiff files generated by the LSM pipeline into a Zarr.</p> <p>Example input files can be found at https://lincbrain.org/dandiset/000004/0.240319.1924/files?location=derivatives%2F</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/lsm/mosaic/#linc_convert.modalities.lsm.mosaic.convert","title":"<code>convert(inp, out=None, *, chunk=128, compressor='blosc', compressor_opt='{}', max_load=512, nii=False, orientation='coronal', center=True, thickness=None, voxel_size=(1, 1, 1))</code>","text":"<p>Convert a collection of tiff files generated by the LSM pipeline into ZARR.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/lsm/mosaic/#linc_convert.modalities.lsm.mosaic.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and     indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and     indicates the anatomical meaning of the bottom of the jp2 image.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/modalities/lsm/mosaic/#linc_convert.modalities.lsm.mosaic.convert--parameters","title":"Parameters","text":"<p>inp     Path to the root directory, which contains a collection of     subfolders named <code>*_z{:02d}_y{:02d}*</code>, each containing a     collection of files named <code>*_plane{:03d}_c{:d}.tiff</code>. out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size when building pyramid nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\". orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center voxel_size     Voxel size along the X, Y and Z dimension, in micron. Source code in <code>linc_convert/modalities/lsm/mosaic.py</code> <pre><code>@mosaic.default\ndef convert(\n    inp: str,\n    out: str = None,\n    *,\n    chunk: int = 128,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 512,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n    voxel_size: list[float] = (1, 1, 1),\n) -&gt; None:\n    \"\"\"\n    Convert a collection of tiff files generated by the LSM pipeline into ZARR.\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n        indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n        indicates the anatomical meaning of the _bottom_ of the jp2 image.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the root directory, which contains a collection of\n        subfolders named `*_z{:02d}_y{:02d}*`, each containing a\n        collection of files named `*_plane{:03d}_c{:d}.tiff`.\n    out\n        Path to the output Zarr directory [&lt;INP&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size when building pyramid\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\".\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    voxel_size\n        Voxel size along the X, Y and Z dimension, in micron.\n    \"\"\"\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    if max_load % 2:\n        max_load += 1\n\n    CHUNK_PATTERN = re.compile(\n        r\"^(?P&lt;prefix&gt;\\w*)\" r\"_z(?P&lt;z&gt;[0-9]+)\" r\"_y(?P&lt;y&gt;[0-9]+)\" r\"(?P&lt;suffix&gt;\\w*)$\"\n    )\n\n    all_chunks_dirnames = list(sorted(glob(os.path.join(inp, \"*_z*_y*\"))))\n    all_chunks_info = dict(\n        dirname=[],\n        prefix=[],\n        suffix=[],\n        z=[],\n        y=[],\n        planes=[\n            dict(\n                fname=[],\n                z=[],\n                c=[],\n                yx_shape=[],\n            )\n            for _ in range(len(all_chunks_dirnames))\n        ],\n    )\n\n    # parse all directory names\n    for dirname in all_chunks_dirnames:\n        parsed = CHUNK_PATTERN.fullmatch(os.path.basename(dirname))\n        all_chunks_info[\"dirname\"].append(dirname)\n        all_chunks_info[\"prefix\"].append(parsed.group(\"prefix\"))\n        all_chunks_info[\"suffix\"].append(parsed.group(\"suffix\"))\n        all_chunks_info[\"z\"].append(int(parsed.group(\"z\")))\n        all_chunks_info[\"y\"].append(int(parsed.group(\"y\")))\n\n    # default output name\n    if not out:\n        out = all_chunks_info[\"prefix\"][0] + all_chunks_info[\"suffix\"][0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    # parse all individual file names\n    nchunkz = max(all_chunks_info[\"z\"])\n    nchunky = max(all_chunks_info[\"y\"])\n    allshapes = [[(0, 0, 0) for _ in range(nchunky)] for _ in range(nchunkz)]\n    nchannels = 0\n    dtype = None\n    for zchunk in range(nchunkz):\n        for ychunk in range(nchunky):\n            for i in range(len(all_chunks_info[\"dirname\"])):\n                if (\n                    all_chunks_info[\"z\"][i] == zchunk + 1\n                    and all_chunks_info[\"y\"][i] == ychunk + 1\n                ):\n                    break\n            dirname = all_chunks_info[\"dirname\"][i]\n            planes_filenames = list(sorted(glob(os.path.join(dirname, \"*.tiff\"))))\n\n            PLANE_PATTERN = re.compile(\n                os.path.basename(dirname) + r\"_plane(?P&lt;z&gt;[0-9]+)\"\n                r\"_c(?P&lt;c&gt;[0-9]+)\"\n                r\".tiff$\"\n            )\n\n            for fname in planes_filenames:\n                parsed = PLANE_PATTERN.fullmatch(os.path.basename(fname))\n                all_chunks_info[\"planes\"][i][\"fname\"] += [fname]\n                all_chunks_info[\"planes\"][i][\"z\"] += [int(parsed.group(\"z\"))]\n                all_chunks_info[\"planes\"][i][\"c\"] += [int(parsed.group(\"c\"))]\n\n                f = TiffFile(fname)\n                dtype = f.pages[0].dtype\n                yx_shape = f.pages[0].shape\n                all_chunks_info[\"planes\"][i][\"yx_shape\"].append(yx_shape)\n\n            nplanes = max(all_chunks_info[\"planes\"][i][\"z\"])\n            nchannels = max(nchannels, max(all_chunks_info[\"planes\"][i][\"c\"]))\n\n            yx_shape = set(all_chunks_info[\"planes\"][i][\"yx_shape\"])\n            if not len(yx_shape) == 1:\n                raise ValueError(\"Incompatible chunk shapes\")\n            yx_shape = list(yx_shape)[0]\n            allshapes[zchunk][ychunk] = (nplanes, *yx_shape)\n\n    # check that all chink shapes are compatible\n    for zchunk in range(nchunkz):\n        if len(set(shape[1] for shape in allshapes[zchunk])) != 1:\n            raise ValueError(\"Incompatible Y shapes\")\n    for ychunk in range(nchunky):\n        if len(set(shape[ychunk][0] for shape in allshapes)) != 1:\n            raise ValueError(\"Incompatible Z shapes\")\n    if len(set(shape[2] for subshapes in allshapes for shape in subshapes)) != 1:\n        raise ValueError(\"Incompatible X shapes\")\n\n    # compute full shape\n    fullshape = [0, 0, 0]\n    fullshape[0] = sum(shape[0][0] for shape in allshapes)\n    fullshape[1] = sum(shape[1] for shape in allshapes[0])\n    fullshape[2] = allshapes[0][0][2]\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": [nchannels] + [chunk] * 3,\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": np.dtype(dtype).str,\n        \"fill_value\": None,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n\n    # write first level\n    omz.create_dataset(\"0\", shape=[nchannels, *fullshape], **opt)\n    array = omz[\"0\"]\n    print(\"Write level 0 with shape\", [nchannels, *fullshape])\n    for i, dirname in enumerate(all_chunks_info[\"dirname\"]):\n        chunkz = all_chunks_info[\"z\"][i] - 1\n        chunky = all_chunks_info[\"y\"][i] - 1\n        planes = all_chunks_info[\"planes\"][i]\n        for j, fname in enumerate(planes[\"fname\"]):\n            subz = planes[\"z\"][j] - 1\n            subc = planes[\"c\"][j] - 1\n            yx_shape = planes[\"yx_shape\"][j]\n\n            zstart = sum(shape[0][0] for shape in allshapes[:chunkz])\n            ystart = sum(\n                shape[1] for subshapes in allshapes for shape in subshapes[:chunky]\n            )\n            print(\n                f\"Write plane \"\n                f\"({subc}, {zstart + subz}, {ystart}:{ystart + yx_shape[0]})\",\n                end=\"\\r\",\n            )\n            slicer = (\n                subc,\n                zstart + subz,\n                slice(ystart, ystart + yx_shape[0]),\n                slice(None),\n            )\n\n            f = TiffFile(fname)\n            array[slicer] = f.asarray()\n    print(\"\")\n\n    # build pyramid using median windows\n    level = 0\n    while any(x &gt; 1 for x in omz[str(level)].shape[-3:]):\n        prev_array = omz[str(level)]\n        prev_shape = prev_array.shape[-3:]\n        level += 1\n\n        new_shape = list(map(lambda x: max(1, x // 2), prev_shape))\n        if all(x &lt; chunk for x in new_shape):\n            break\n        print(\"Compute level\", level, \"with shape\", new_shape)\n        omz.create_dataset(str(level), shape=[nchannels, *new_shape], **opt)\n        new_array = omz[str(level)]\n\n        nz, ny, nx = prev_array.shape[-3:]\n        ncz = ceildiv(nz, max_load)\n        ncy = ceildiv(ny, max_load)\n        ncx = ceildiv(nx, max_load)\n\n        for cz in range(ncz):\n            for cy in range(ncy):\n                for cx in range(ncx):\n                    print(f\"chunk ({cz}, {cy}, {cx}) / ({ncz}, {ncy}, {ncx})\", end=\"\\r\")\n\n                    dat = prev_array[\n                        ...,\n                        cz * max_load : (cz + 1) * max_load,\n                        cy * max_load : (cy + 1) * max_load,\n                        cx * max_load : (cx + 1) * max_load,\n                    ]\n                    crop = [0 if x == 1 else x % 2 for x in dat.shape[-3:]]\n                    slicer = [slice(-1) if x else slice(None) for x in crop]\n                    dat = dat[(Ellipsis, *slicer)]\n                    pz, py, px = dat.shape[-3:]\n\n                    dat = dat.reshape(\n                        [\n                            nchannels,\n                            max(pz // 2, 1),\n                            min(pz, 2),\n                            max(py // 2, 1),\n                            min(py, 2),\n                            max(px // 2, 1),\n                            min(px, 2),\n                        ]\n                    )\n                    dat = dat.transpose([0, 1, 3, 5, 2, 4, 6])\n                    dat = dat.reshape(\n                        [\n                            nchannels,\n                            max(pz // 2, 1),\n                            max(py // 2, 1),\n                            max(px // 2, 1),\n                            -1,\n                        ]\n                    )\n                    dat = np.median(dat, -1)\n\n                    new_array[\n                        ...,\n                        cz * max_load // 2 : (cz + 1) * max_load // 2,\n                        cy * max_load // 2 : (cy + 1) * max_load // 2,\n                        cx * max_load // 2 : (cx + 1) * max_load // 2,\n                    ] = dat\n\n    print(\"\")\n    nblevel = level\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"z\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"y\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"median window 2x2x2\",\n            \"name\": \"\",\n        }\n    ]\n    multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    voxel_size = list(map(float, reversed(voxel_size)))\n    factor = [1] * 3\n    for n in range(nblevel):\n        shape = omz[str(n)].shape[-3:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # We made sure that the downsampling level is exactly 2\n        # However, once a dimension has size 1, we stop downsampling.\n        if n &gt; 0:\n            shape_prev = omz[str(n - 1)].shape[-3:]\n            if shape_prev[0] != shape[0]:\n                factor[0] *= 2\n            if shape_prev[1] != shape[1]:\n                factor[1] *= 2\n            if shape_prev[2] != shape[2]:\n                factor[2] *= 2\n\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0]\n                + [\n                    factor[0] * voxel_size[0],\n                    factor[1] * voxel_size[1],\n                    factor[2] * voxel_size[2],\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0]\n                + [\n                    (factor[0] - 1) * voxel_size[0] * 0.5,\n                    (factor[1] - 1) * voxel_size[1] * 0.5,\n                    (factor[2] - 1) * voxel_size[2] * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * 4, \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    if not nii:\n        print(\"done.\")\n        return\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    shape = shape[:3] + [1] + shape[3:]  # insert time dimension\n    affine = orientation_to_affine(orientation, *voxel_size)\n    if center:\n        affine = center_affine(affine, shape[:3])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"nz2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n    print(\"done.\")\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/__init__/","title":"init","text":"<p>Various utilities.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/j2k/","title":"J2k","text":"<p>Utilities for JPEG2000 files.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K","title":"<code>WrappedJ2K</code>  <code>dataclass</code>","text":"<p>Array-like wrapper around a JPEG2000 object.</p> <p>A wrapper around the J2K object at any resolution level, and with virtual transposition of the axes into [C, H, W] order.</p> <p>The resulting object can be sliced, but each index must be a <code>slice</code> (dropping axes using integer indices or adding axes using <code>None</code> indices is forbidden).</p> <p>The point is to ensure that the zarr writer only loads chunk-sized data.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K--parameters","title":"Parameters","text":"<p>j2k : glymur.Jp2k     The JPEG2000 object. level : int     Resolution level to map (highest resolution = 0). channel_first : bool     Return an array with shape (C, H, W) instead of (H, W, C)     when there is a channel dimension.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>@dataclass\nclass WrappedJ2K:\n    \"\"\"\n    Array-like wrapper around a JPEG2000 object.\n\n    A wrapper around the J2K object at any resolution level, and\n    with virtual transposition of the axes into [C, H, W] order.\n\n    The resulting object can be sliced, but each index must be a `slice`\n    (dropping axes using integer indices or adding axes using `None`\n    indices is forbidden).\n\n    The point is to ensure that the zarr writer only loads chunk-sized data.\n\n    Parameters\n    ----------\n    j2k : glymur.Jp2k\n        The JPEG2000 object.\n    level : int\n        Resolution level to map (highest resolution = 0).\n    channel_first : bool\n        Return an array with shape (C, H, W) instead of (H, W, C)\n        when there is a channel dimension.\n    \"\"\"\n\n    j2k: Jp2k\n    level: int = 0\n    channel_first: bool = True\n\n    @property\n    def shape(self) -&gt; tuple[int]:\n        \"\"\"Shape of the current level.\"\"\"\n        channel = list(self.j2k.shape[2:])\n        shape = [ceildiv(s, 2**self.level) for s in self.j2k.shape[:2]]\n        if self.channel_first:\n            shape = channel + shape\n        else:\n            shape += channel\n        return tuple(shape)\n\n    @property\n    def dtype(self) -&gt; np.dtype:\n        \"\"\"Data type of the wrapped image.\"\"\"\n        return self.j2k.dtype\n\n    def __getitem__(self, index: tuple[slice] | slice) -&gt; np.ndarray:\n        \"\"\"Multidimensional slicing of the wrapped array.\"\"\"\n        if not isinstance(index, tuple):\n            index = (index,)\n        if Ellipsis not in index:\n            index += (Ellipsis,)\n        if any(idx is None for idx in index):\n            raise TypeError(\"newaxis not supported\")\n\n        # substitute ellipses\n        new_index = []\n        has_seen_ellipsis = False\n        last_was_ellipsis = False\n        nb_ellipsis = max(0, self.j2k.ndim + 1 - len(index))\n        for idx in index:\n            if idx is Ellipsis:\n                if not has_seen_ellipsis:\n                    new_index += [slice(None)] * nb_ellipsis\n                elif not last_was_ellipsis:\n                    raise ValueError(\"Multiple ellipses should be contiguous\")\n                has_seen_ellipsis = True\n                last_was_ellipsis = True\n            elif not isinstance(idx, slice):\n                raise TypeError(\"Only slices are supported\")\n            elif idx.step not in (None, 1):\n                raise ValueError(\"Striding not supported\")\n            else:\n                last_was_ellipsis = False\n                new_index += [idx]\n        index = new_index\n\n        if self.channel_first:\n            *cidx, hidx, widx = index\n        else:\n            hidx, widx, *cidx = index\n        hstart, hstop = hidx.start or 0, hidx.stop or 0\n        wstart, wstop = widx.start or 0, widx.stop or 0\n\n        # convert to level 0 indices\n        hstart *= 2**self.level\n        hstop *= 2**self.level\n        wstart *= 2**self.level\n        wstop *= 2**self.level\n        hstop = min(hstop or self.j2k.shape[0], self.j2k.shape[0])\n        wstop = min(wstop or self.j2k.shape[1], self.j2k.shape[1])\n        area = (hstart, wstart, hstop, wstop)\n\n        data = self.j2k.read(rlevel=self.level, area=area)\n        if cidx:\n            data = data[:, :, cidx[0]]\n            if self.channel_first:\n                data = np.transpose(data, [2, 0, 1])\n        return data\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K.dtype","title":"<code>dtype: np.dtype</code>  <code>property</code>","text":"<p>Data type of the wrapped image.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K.shape","title":"<code>shape: tuple[int]</code>  <code>property</code>","text":"<p>Shape of the current level.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Multidimensional slicing of the wrapped array.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>def __getitem__(self, index: tuple[slice] | slice) -&gt; np.ndarray:\n    \"\"\"Multidimensional slicing of the wrapped array.\"\"\"\n    if not isinstance(index, tuple):\n        index = (index,)\n    if Ellipsis not in index:\n        index += (Ellipsis,)\n    if any(idx is None for idx in index):\n        raise TypeError(\"newaxis not supported\")\n\n    # substitute ellipses\n    new_index = []\n    has_seen_ellipsis = False\n    last_was_ellipsis = False\n    nb_ellipsis = max(0, self.j2k.ndim + 1 - len(index))\n    for idx in index:\n        if idx is Ellipsis:\n            if not has_seen_ellipsis:\n                new_index += [slice(None)] * nb_ellipsis\n            elif not last_was_ellipsis:\n                raise ValueError(\"Multiple ellipses should be contiguous\")\n            has_seen_ellipsis = True\n            last_was_ellipsis = True\n        elif not isinstance(idx, slice):\n            raise TypeError(\"Only slices are supported\")\n        elif idx.step not in (None, 1):\n            raise ValueError(\"Striding not supported\")\n        else:\n            last_was_ellipsis = False\n            new_index += [idx]\n    index = new_index\n\n    if self.channel_first:\n        *cidx, hidx, widx = index\n    else:\n        hidx, widx, *cidx = index\n    hstart, hstop = hidx.start or 0, hidx.stop or 0\n    wstart, wstop = widx.start or 0, widx.stop or 0\n\n    # convert to level 0 indices\n    hstart *= 2**self.level\n    hstop *= 2**self.level\n    wstart *= 2**self.level\n    wstop *= 2**self.level\n    hstop = min(hstop or self.j2k.shape[0], self.j2k.shape[0])\n    wstop = min(wstop or self.j2k.shape[1], self.j2k.shape[1])\n    area = (hstart, wstart, hstop, wstop)\n\n    data = self.j2k.read(rlevel=self.level, area=area)\n    if cidx:\n        data = data[:, :, cidx[0]]\n        if self.channel_first:\n            data = np.transpose(data, [2, 0, 1])\n    return data\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/j2k/#linc_convert.utils.j2k.get_pixelsize","title":"<code>get_pixelsize(j2k)</code>","text":"<p>Read pixelsize from the JPEG2000 file.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>def get_pixelsize(j2k: Jp2k) -&gt; tuple[float, float]:\n    \"\"\"Read pixelsize from the JPEG2000 file.\"\"\"\n    # Adobe XMP metadata\n    # https://en.wikipedia.org/wiki/Extensible_Metadata_Platform\n    XMP_UUID = \"BE7ACFCB97A942E89C71999491E3AFAC\"\n    TAG_Images = \"{http://ns.adobe.com/xap/1.0/}Images\"\n    Tag_Desc = \"{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Description\"\n    Tag_PixelWidth = \"{http://ns.adobe.com/xap/1.0/}PixelWidth\"\n    Tag_PixelHeight = \"{http://ns.adobe.com/xap/1.0/}PixelHeight\"\n\n    vxw = vxh = 1.0\n    for box in j2k.box:\n        if getattr(box, \"uuid\", None) == uuid.UUID(XMP_UUID):\n            try:\n                images = list(box.data.iter(TAG_Images))[0]\n                desc = list(images.iter(Tag_Desc))[0]\n                vxw = float(desc.attrib[Tag_PixelWidth])\n                vxh = float(desc.attrib[Tag_PixelHeight])\n            except Exception:\n                pass\n    return vxw, vxh\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/math/","title":"Math","text":"<p>Math utilities.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/math/#linc_convert.utils.math.ceildiv","title":"<code>ceildiv(x, y)</code>","text":"<p>Ceil of ratio of two numbers.</p> Source code in <code>linc_convert/utils/math.py</code> <pre><code>def ceildiv(x: Number, y: Number) -&gt; int:\n    \"\"\"Ceil of ratio of two numbers.\"\"\"\n    return int(math.ceil(x / y))\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/math/#linc_convert.utils.math.floordiv","title":"<code>floordiv(x, y)</code>","text":"<p>Floor of ratio of two numbers.</p> Source code in <code>linc_convert/utils/math.py</code> <pre><code>def floordiv(x: Number, y: Number) -&gt; int:\n    \"\"\"Floor of ratio of two numbers.\"\"\"\n    return int(math.floor(x / y))\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/","title":"Orientation","text":"<p>Orientation of an array of voxels with respect to world space.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.center_affine","title":"<code>center_affine(affine, shape)</code>","text":"<p>Ensure that the center of the field-of-view has world coordinate (0,0,0).</p> <p>The input affine is NOT modified in-place</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.center_affine--parameters","title":"Parameters","text":"<p>affine : array     Orientation affine matrix shape : list[int]     Shape of the array of voxels</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.center_affine--returns","title":"Returns","text":"<p>affine : array     Modified affine matrix.</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def center_affine(affine: np.ndarray, shape: list[int]) -&gt; np.ndarray:\n    \"\"\"\n    Ensure that the center of the field-of-view has world coordinate (0,0,0).\n\n    !!! note \"The input affine is NOT modified in-place\"\n\n    Parameters\n    ----------\n    affine : array\n        Orientation affine matrix\n    shape : list[int]\n        Shape of the array of voxels\n\n    Returns\n    -------\n    affine : array\n        Modified affine matrix.\n    \"\"\"\n    if len(shape) == 2:\n        shape = [*shape, 1]\n    shape = np.asarray(shape)\n    affine = np.copy(affine)\n    affine[:3, -1] = -0.5 * affine[:3, :3] @ (shape - 1)\n    return affine\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.orientation_ensure_3d","title":"<code>orientation_ensure_3d(orientation)</code>","text":"<p>Convert an ND orientation string to a 3D orientation string.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.orientation_ensure_3d--parameters","title":"Parameters","text":"<p>orientation : str     A 2D or 3D orientation string, such as <code>\"RA\"</code> or <code>\"RAS\"</code>.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.orientation_ensure_3d--returns","title":"Returns","text":"<p>orientation : str     A 3D orientation string compatible with the input orientaition</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def orientation_ensure_3d(orientation: str) -&gt; str:\n    \"\"\"\n    Convert an ND orientation string to a 3D orientation string.\n\n    Parameters\n    ----------\n    orientation : str\n        A 2D or 3D orientation string, such as `\"RA\"` or `\"RAS\"`.\n\n    Returns\n    -------\n    orientation : str\n        A 3D orientation string compatible with the input orientaition\n    \"\"\"\n    orientation = {\n        \"coronal\": \"LI\",\n        \"axial\": \"LP\",\n        \"sagittal\": \"PI\",\n    }.get(orientation.lower(), orientation).upper()\n    if len(orientation) == 2:\n        if \"L\" not in orientation and \"R\" not in orientation:\n            orientation += \"R\"\n        if \"P\" not in orientation and \"A\" not in orientation:\n            orientation += \"A\"\n        if \"I\" not in orientation and \"S\" not in orientation:\n            orientation += \"S\"\n    return orientation\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.orientation_to_affine","title":"<code>orientation_to_affine(orientation, vxw=1, vxh=1, vxd=1)</code>","text":"<p>Build an affine matrix from an orientation string and voxel size.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.orientation_to_affine--parameters","title":"Parameters","text":"<p>orientation : str     Orientation string vxw : float     Width voxel size vxh : float     Height voxel size vxd : float     Depth voxel size</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/orientation/#linc_convert.utils.orientation.orientation_to_affine--returns","title":"Returns","text":"<p>affine : (4, 4) array     Affine orientation matrix</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def orientation_to_affine(\n    orientation: str, vxw: float = 1, vxh: float = 1, vxd: float = 1\n) -&gt; np.ndarray:\n    \"\"\"\n    Build an affine matrix from an orientation string and voxel size.\n\n    Parameters\n    ----------\n    orientation : str\n        Orientation string\n    vxw : float\n        Width voxel size\n    vxh : float\n        Height voxel size\n    vxd : float\n        Depth voxel size\n\n    Returns\n    -------\n    affine : (4, 4) array\n        Affine orientation matrix\n    \"\"\"\n    orientation = orientation_ensure_3d(orientation)\n    affine = np.zeros([4, 4])\n    vx = np.asarray([vxw, vxh, vxd])\n    for i in range(3):\n        letter = orientation[i]\n        sign = -1 if letter in \"LPI\" else 1\n        letter = {\"L\": \"R\", \"P\": \"A\", \"I\": \"S\"}.get(letter, letter)\n        index = list(\"RAS\").index(letter)\n        affine[index, i] = sign * vx[i]\n    return affine\n</code></pre>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/zarr/","title":"Zarr","text":"<p>Zarr utilities.</p>"},{"location":"/Users/kabilar/Documents/GitHub/linc-convert/docs/api/utils/zarr/#linc_convert.utils.zarr.make_compressor","title":"<code>make_compressor(name, **prm)</code>","text":"<p>Build compressor object from name and options.</p> Source code in <code>linc_convert/utils/zarr.py</code> <pre><code>def make_compressor(name: str, **prm: dict) -&gt; numcodecs.abc.Codec:\n    \"\"\"Build compressor object from name and options.\"\"\"\n    # TODO: we should use `numcodecs.get_codec` instead`\n    if not isinstance(name, str):\n        return name\n    name = name.lower()\n    if name == \"blosc\":\n        Compressor = numcodecs.Blosc\n    elif name == \"zlib\":\n        Compressor = numcodecs.Zlib\n    else:\n        raise ValueError(\"Unknown compressor\", name)\n    return Compressor(**prm)\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cli</li> <li>modalities<ul> <li>df<ul> <li>cli</li> <li>multi_slice</li> <li>single_slice</li> </ul> </li> <li>lsm<ul> <li>cli</li> <li>mosaic</li> </ul> </li> </ul> </li> <li>utils<ul> <li>j2k</li> <li>math</li> <li>orientation</li> <li>zarr</li> </ul> </li> </ul>"},{"location":"api/cli/","title":"cli","text":"<p>Root command line entry point.</p>"},{"location":"api/modalities/__init__/","title":"init","text":"<p>Converters for all imaging modalities.</p>"},{"location":"api/modalities/df/__init__/","title":"init","text":"<p>Dark Field microscopy converters.</p>"},{"location":"api/modalities/df/cli/","title":"cli","text":"<p>Entry-points for Dark Field microscopy converter.</p>"},{"location":"api/modalities/df/multi_slice/","title":"multi_slice","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a OME-ZARR pyramid.</p> <p>We do not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"api/modalities/df/multi_slice/#linc_convert.modalities.df.multi_slice.convert","title":"<code>convert(inp, out=None, *, chunk=4096, compressor='blosc', compressor_opt='{}', max_load=16384, nii=False, orientation='coronal', center=True, thickness=None)</code>","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p> <p>This command converts a batch of slices and stacks them together into a single 3D Zarr.</p>"},{"location":"api/modalities/df/multi_slice/#linc_convert.modalities.df.multi_slice.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and   indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and   indicates the anatomical meaning of the bottom of the jp2 image,</li> <li>the third letter corresponds to the slice dimension and   indicates the anatomical meaninff of the end of the stack.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"api/modalities/df/multi_slice/#linc_convert.modalities.df.multi_slice.convert--parameters","title":"Parameters","text":"<p>inp     Path to the input slices out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\" orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center thickness     Slice thickness Source code in <code>linc_convert/modalities/df/multi_slice.py</code> <pre><code>@ms.default\ndef convert(\n    inp: list[str],\n    out: str | None = None,\n    *,\n    chunk: int = 4096,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 16384,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.\n\n    It does not recompute the image pyramid but instead reuse the\n    JPEG2000 levels (obtained by wavelet transform).\n\n    This command converts a batch of slices and stacks them together\n    into a single 3D Zarr.\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n      indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n      indicates the anatomical meaning of the _bottom_ of the jp2 image,\n    * the third letter corresponds to the slice dimension and\n      indicates the anatomical meaninff of the _end_ of the stack.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the input slices\n    out\n        Path to the output Zarr directory [&lt;dirname(INP)&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\"\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    thickness\n        Slice thickness\n    \"\"\"\n    # Default output path\n    if not out:\n        out = os.path.splitext(inp[0])[0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    nblevel, has_channel, dtype_jp2 = float(\"inf\"), float(\"inf\"), \"\"\n\n    # Compute output shape\n    new_height, new_width = 0, 0\n    for inp1 in inp:\n        jp2 = glymur.Jp2k(inp1)\n        nblevel = min(nblevel, jp2.codestream.segment[2].num_res)\n        has_channel = min(has_channel, jp2.ndim - 2)\n        dtype_jp2 = np.dtype(jp2.dtype).str\n        if jp2.shape[0] &gt; new_height:\n            new_height = jp2.shape[0]\n        if jp2.shape[1] &gt; new_width:\n            new_width = jp2.shape[1]\n    new_size = (new_height, new_width)\n    if has_channel:\n        new_size += (3.0,)\n    print(len(inp), new_size, nblevel, has_channel)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": list(new_size[2:]) + [1] + [chunk, chunk],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": dtype_jp2,\n        \"fill_value\": 0,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n    print(opt)\n    print(new_size)\n    # Write each level\n    for level in range(nblevel):\n        shape = [ceildiv(s, 2**level) for s in new_size[:2]]\n        shape = [new_size[2]] + [len(inp)] + shape\n\n        omz.create_dataset(f\"{level}\", shape=shape, **opt)\n        array = omz[f\"{level}\"]\n\n        # Write each slice\n        for idx, inp1 in enumerate(inp):\n            j2k = glymur.Jp2k(inp1)\n            vxw, vxh = get_pixelsize(j2k)\n            subdat = WrappedJ2K(j2k, level=level)\n            subdat_size = subdat.shape\n            print(\n                \"Convert level\",\n                level,\n                \"with shape\",\n                shape,\n                \"for slice\",\n                idx,\n                \"with size\",\n                subdat_size,\n            )\n\n            # offset while attaching\n            x = floordiv(shape[-2] - subdat_size[-2], 2)\n            y = floordiv(shape[-1] - subdat_size[-1], 2)\n\n            if max_load is None or (shape[-2] &lt; max_load and shape[-1] &lt; max_load):\n                array[..., idx, x : x + subdat_size[1], y : y + subdat_size[2]] = (\n                    subdat[...]\n                )\n\n            else:\n                ni = ceildiv(shape[-2], max_load)\n                nj = ceildiv(shape[-1], max_load)\n\n                for i in range(ni):\n                    for j in range(nj):\n                        print(f\"\\r{i+1}/{ni}, {j+1}/{nj}\", end=\" \")\n                        start_x, end_x = (\n                            i * max_load,\n                            min((i + 1) * max_load, shape[-2]),\n                        )\n                        start_y, end_y = (\n                            j * max_load,\n                            min((j + 1) * max_load, shape[-1]),\n                        )\n                        if end_x &lt;= x or end_y &lt;= y:\n                            continue\n\n                        if start_x &gt;= subdat_size[-2] or start_y &gt;= subdat_size[-1]:\n                            continue\n\n                        array[\n                            ...,\n                            idx,\n                            x + start_x : x + min(end_x, subdat_size[-2]),\n                            y + start_y : y + min(end_y, subdat_size[-1]),\n                        ] = subdat[\n                            ...,\n                            start_x : min((i + 1) * max_load, subdat_size[-2]),\n                            start_y : min((j + 1) * max_load, subdat_size[-1]),\n                        ]\n                print(\"\")\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"z\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"y\", \"type\": \"distance\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"jpeg2000\",\n            \"name\": \"\",\n        }\n    ]\n    if has_channel:\n        multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    for n in range(nblevel):\n        shape0 = omz[\"0\"].shape[-2:]\n        shape = omz[str(n)].shape[-2:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # I assume that wavelet transforms end up aligning voxel edges\n        # across levels, so the effective scaling is the shape ratio,\n        # and there is a half voxel shift wrt to the \"center of first voxel\"\n        # frame\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0] * has_channel\n                + [\n                    1.0,\n                    (shape0[0] / shape[0]) * vxh,\n                    (shape0[1] / shape[1]) * vxw,\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0] * has_channel\n                + [\n                    0.0,\n                    (shape0[0] / shape[0] - 1) * vxh * 0.5,\n                    (shape0[1] / shape[1] - 1) * vxw * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * (3 + has_channel), \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    if has_channel:\n        shape = shape[:3] + [1] + shape[3:]\n    affine = orientation_to_affine(orientation, vxw, vxh, thickness or 1)\n    if center:\n        affine = center_affine(affine, shape[:2])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"n+2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n\n    # Write sidecar .json file\n    json_name = os.path.splitext(out)[0]\n    json_name += \".json\"\n    dic = {}\n    dic[\"PixelSize\"] = json.dumps([vxw, vxh])\n    dic[\"PixelSizeUnits\"] = \"um\"\n    dic[\"SliceThickness\"] = 1.2\n    dic[\"SliceThicknessUnits\"] = \"mm\"\n    dic[\"SampleStaining\"] = \"LY\"\n\n    with open(json_name, \"w\") as outfile:\n        json.dump(dic, outfile)\n        outfile.write(\"\\n\")\n\n    print(\"done.\")\n</code></pre>"},{"location":"api/modalities/df/single_slice/","title":"single_slice","text":"<p>Converts JPEG2000 files generated by MBF-Neurolucida into a OME-ZARR pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"api/modalities/df/single_slice/#linc_convert.modalities.df.single_slice.convert","title":"<code>convert(inp, out=None, *, chunk=1024, compressor='blosc', compressor_opt='{}', max_load=16384, nii=False, orientation='coronal', center=True, thickness=None)</code>","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"api/modalities/df/single_slice/#linc_convert.modalities.df.single_slice.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and     indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and     indicates the anatomical meaning of the bottom of the jp2 image.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"api/modalities/df/single_slice/#linc_convert.modalities.df.single_slice.convert--parameters","title":"Parameters","text":"<p>inp     Path to the input JP2 file out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\" orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center thickness     Slice thickness Source code in <code>linc_convert/modalities/df/single_slice.py</code> <pre><code>@ss.default\ndef convert(\n    inp: str,\n    out: str | None = None,\n    *,\n    chunk: int = 1024,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 16384,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.\n\n    It does not recompute the image pyramid but instead reuse the JPEG2000\n    levels (obtained by wavelet transform).\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n        indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n        indicates the anatomical meaning of the _bottom_ of the jp2 image.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the input JP2 file\n    out\n        Path to the output Zarr directory [&lt;INP&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\"\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    thickness\n        Slice thickness\n    \"\"\"\n    if not out:\n        out = os.path.splitext(inp)[0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    j2k = glymur.Jp2k(inp)\n    vxw, vxh = get_pixelsize(j2k)\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": list(j2k.shape[2:]) + [chunk, chunk],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": np.dtype(j2k.dtype).str,\n        \"fill_value\": None,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n\n    # Write each level\n    nblevel = j2k.codestream.segment[2].num_res\n    has_channel = j2k.ndim - 2\n    for level in range(nblevel):\n        subdat = WrappedJ2K(j2k, level=level)\n        shape = subdat.shape\n        print(\"Convert level\", level, \"with shape\", shape)\n        omz.create_dataset(str(level), shape=shape, **opt)\n        array = omz[str(level)]\n        if max_load is None or (shape[-2] &lt; max_load and shape[-1] &lt; max_load):\n            array[...] = subdat[...]\n        else:\n            ni = ceildiv(shape[-2], max_load)\n            nj = ceildiv(shape[-1], max_load)\n            for i in range(ni):\n                for j in range(nj):\n                    print(f\"\\r{i+1}/{ni}, {j+1}/{nj}\", end=\"\")\n                    array[\n                        ...,\n                        i * max_load : min((i + 1) * max_load, shape[-2]),\n                        j * max_load : min((j + 1) * max_load, shape[-1]),\n                    ] = subdat[\n                        ...,\n                        i * max_load : min((i + 1) * max_load, shape[-2]),\n                        j * max_load : min((j + 1) * max_load, shape[-1]),\n                    ]\n            print(\"\")\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"y\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"jpeg2000\",\n            \"name\": \"\",\n        }\n    ]\n    if has_channel:\n        multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    for n in range(nblevel):\n        shape0 = omz[\"0\"].shape[-2:]\n        shape = omz[str(n)].shape[-2:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # I assume that wavelet transforms end up aligning voxel edges\n        # across levels, so the effective scaling is the shape ratio,\n        # and there is a half voxel shift wrt to the \"center of first voxel\"\n        # frame\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0] * has_channel\n                + [\n                    (shape0[0] / shape[0]) * vxh,\n                    (shape0[1] / shape[1]) * vxw,\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0] * has_channel\n                + [\n                    (shape0[0] / shape[0] - 1) * vxh * 0.5,\n                    (shape0[1] / shape[1] - 1) * vxw * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * (2 + has_channel), \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    if not nii:\n        print(\"done.\")\n        return\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    if has_channel:\n        shape = shape[:2] + [1, 1] + shape[2:]\n    affine = orientation_to_affine(orientation, vxw, vxh, thickness or 1)\n    if center:\n        affine = center_affine(affine, shape[:2])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"n+2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n    print(\"done.\")\n</code></pre>"},{"location":"api/modalities/lsm/__init__/","title":"init","text":"<p>Light Sheet Microscopy converters.</p>"},{"location":"api/modalities/lsm/cli/","title":"cli","text":"<p>Entry-points for Dark Field microscopy converter.</p>"},{"location":"api/modalities/lsm/mosaic/","title":"mosaic","text":"<p>Convert a collection of tiff files generated by the LSM pipeline into a Zarr.</p> <p>Example input files can be found at https://lincbrain.org/dandiset/000004/0.240319.1924/files?location=derivatives%2F</p>"},{"location":"api/modalities/lsm/mosaic/#linc_convert.modalities.lsm.mosaic.convert","title":"<code>convert(inp, out=None, *, chunk=128, compressor='blosc', compressor_opt='{}', max_load=512, nii=False, orientation='coronal', center=True, thickness=None, voxel_size=(1, 1, 1))</code>","text":"<p>Convert a collection of tiff files generated by the LSM pipeline into ZARR.</p>"},{"location":"api/modalities/lsm/mosaic/#linc_convert.modalities.lsm.mosaic.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and     indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and     indicates the anatomical meaning of the bottom of the jp2 image.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"api/modalities/lsm/mosaic/#linc_convert.modalities.lsm.mosaic.convert--parameters","title":"Parameters","text":"<p>inp     Path to the root directory, which contains a collection of     subfolders named <code>*_z{:02d}_y{:02d}*</code>, each containing a     collection of files named <code>*_plane{:03d}_c{:d}.tiff</code>. out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size when building pyramid nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\". orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center voxel_size     Voxel size along the X, Y and Z dimension, in micron. Source code in <code>linc_convert/modalities/lsm/mosaic.py</code> <pre><code>@mosaic.default\ndef convert(\n    inp: str,\n    out: str = None,\n    *,\n    chunk: int = 128,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 512,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n    voxel_size: list[float] = (1, 1, 1),\n) -&gt; None:\n    \"\"\"\n    Convert a collection of tiff files generated by the LSM pipeline into ZARR.\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n        indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n        indicates the anatomical meaning of the _bottom_ of the jp2 image.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the root directory, which contains a collection of\n        subfolders named `*_z{:02d}_y{:02d}*`, each containing a\n        collection of files named `*_plane{:03d}_c{:d}.tiff`.\n    out\n        Path to the output Zarr directory [&lt;INP&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size when building pyramid\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\".\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    voxel_size\n        Voxel size along the X, Y and Z dimension, in micron.\n    \"\"\"\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    if max_load % 2:\n        max_load += 1\n\n    CHUNK_PATTERN = re.compile(\n        r\"^(?P&lt;prefix&gt;\\w*)\" r\"_z(?P&lt;z&gt;[0-9]+)\" r\"_y(?P&lt;y&gt;[0-9]+)\" r\"(?P&lt;suffix&gt;\\w*)$\"\n    )\n\n    all_chunks_dirnames = list(sorted(glob(os.path.join(inp, \"*_z*_y*\"))))\n    all_chunks_info = dict(\n        dirname=[],\n        prefix=[],\n        suffix=[],\n        z=[],\n        y=[],\n        planes=[\n            dict(\n                fname=[],\n                z=[],\n                c=[],\n                yx_shape=[],\n            )\n            for _ in range(len(all_chunks_dirnames))\n        ],\n    )\n\n    # parse all directory names\n    for dirname in all_chunks_dirnames:\n        parsed = CHUNK_PATTERN.fullmatch(os.path.basename(dirname))\n        all_chunks_info[\"dirname\"].append(dirname)\n        all_chunks_info[\"prefix\"].append(parsed.group(\"prefix\"))\n        all_chunks_info[\"suffix\"].append(parsed.group(\"suffix\"))\n        all_chunks_info[\"z\"].append(int(parsed.group(\"z\")))\n        all_chunks_info[\"y\"].append(int(parsed.group(\"y\")))\n\n    # default output name\n    if not out:\n        out = all_chunks_info[\"prefix\"][0] + all_chunks_info[\"suffix\"][0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    # parse all individual file names\n    nchunkz = max(all_chunks_info[\"z\"])\n    nchunky = max(all_chunks_info[\"y\"])\n    allshapes = [[(0, 0, 0) for _ in range(nchunky)] for _ in range(nchunkz)]\n    nchannels = 0\n    dtype = None\n    for zchunk in range(nchunkz):\n        for ychunk in range(nchunky):\n            for i in range(len(all_chunks_info[\"dirname\"])):\n                if (\n                    all_chunks_info[\"z\"][i] == zchunk + 1\n                    and all_chunks_info[\"y\"][i] == ychunk + 1\n                ):\n                    break\n            dirname = all_chunks_info[\"dirname\"][i]\n            planes_filenames = list(sorted(glob(os.path.join(dirname, \"*.tiff\"))))\n\n            PLANE_PATTERN = re.compile(\n                os.path.basename(dirname) + r\"_plane(?P&lt;z&gt;[0-9]+)\"\n                r\"_c(?P&lt;c&gt;[0-9]+)\"\n                r\".tiff$\"\n            )\n\n            for fname in planes_filenames:\n                parsed = PLANE_PATTERN.fullmatch(os.path.basename(fname))\n                all_chunks_info[\"planes\"][i][\"fname\"] += [fname]\n                all_chunks_info[\"planes\"][i][\"z\"] += [int(parsed.group(\"z\"))]\n                all_chunks_info[\"planes\"][i][\"c\"] += [int(parsed.group(\"c\"))]\n\n                f = TiffFile(fname)\n                dtype = f.pages[0].dtype\n                yx_shape = f.pages[0].shape\n                all_chunks_info[\"planes\"][i][\"yx_shape\"].append(yx_shape)\n\n            nplanes = max(all_chunks_info[\"planes\"][i][\"z\"])\n            nchannels = max(nchannels, max(all_chunks_info[\"planes\"][i][\"c\"]))\n\n            yx_shape = set(all_chunks_info[\"planes\"][i][\"yx_shape\"])\n            if not len(yx_shape) == 1:\n                raise ValueError(\"Incompatible chunk shapes\")\n            yx_shape = list(yx_shape)[0]\n            allshapes[zchunk][ychunk] = (nplanes, *yx_shape)\n\n    # check that all chink shapes are compatible\n    for zchunk in range(nchunkz):\n        if len(set(shape[1] for shape in allshapes[zchunk])) != 1:\n            raise ValueError(\"Incompatible Y shapes\")\n    for ychunk in range(nchunky):\n        if len(set(shape[ychunk][0] for shape in allshapes)) != 1:\n            raise ValueError(\"Incompatible Z shapes\")\n    if len(set(shape[2] for subshapes in allshapes for shape in subshapes)) != 1:\n        raise ValueError(\"Incompatible X shapes\")\n\n    # compute full shape\n    fullshape = [0, 0, 0]\n    fullshape[0] = sum(shape[0][0] for shape in allshapes)\n    fullshape[1] = sum(shape[1] for shape in allshapes[0])\n    fullshape[2] = allshapes[0][0][2]\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": [nchannels] + [chunk] * 3,\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": np.dtype(dtype).str,\n        \"fill_value\": None,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n\n    # write first level\n    omz.create_dataset(\"0\", shape=[nchannels, *fullshape], **opt)\n    array = omz[\"0\"]\n    print(\"Write level 0 with shape\", [nchannels, *fullshape])\n    for i, dirname in enumerate(all_chunks_info[\"dirname\"]):\n        chunkz = all_chunks_info[\"z\"][i] - 1\n        chunky = all_chunks_info[\"y\"][i] - 1\n        planes = all_chunks_info[\"planes\"][i]\n        for j, fname in enumerate(planes[\"fname\"]):\n            subz = planes[\"z\"][j] - 1\n            subc = planes[\"c\"][j] - 1\n            yx_shape = planes[\"yx_shape\"][j]\n\n            zstart = sum(shape[0][0] for shape in allshapes[:chunkz])\n            ystart = sum(\n                shape[1] for subshapes in allshapes for shape in subshapes[:chunky]\n            )\n            print(\n                f\"Write plane \"\n                f\"({subc}, {zstart + subz}, {ystart}:{ystart + yx_shape[0]})\",\n                end=\"\\r\",\n            )\n            slicer = (\n                subc,\n                zstart + subz,\n                slice(ystart, ystart + yx_shape[0]),\n                slice(None),\n            )\n\n            f = TiffFile(fname)\n            array[slicer] = f.asarray()\n    print(\"\")\n\n    # build pyramid using median windows\n    level = 0\n    while any(x &gt; 1 for x in omz[str(level)].shape[-3:]):\n        prev_array = omz[str(level)]\n        prev_shape = prev_array.shape[-3:]\n        level += 1\n\n        new_shape = list(map(lambda x: max(1, x // 2), prev_shape))\n        if all(x &lt; chunk for x in new_shape):\n            break\n        print(\"Compute level\", level, \"with shape\", new_shape)\n        omz.create_dataset(str(level), shape=[nchannels, *new_shape], **opt)\n        new_array = omz[str(level)]\n\n        nz, ny, nx = prev_array.shape[-3:]\n        ncz = ceildiv(nz, max_load)\n        ncy = ceildiv(ny, max_load)\n        ncx = ceildiv(nx, max_load)\n\n        for cz in range(ncz):\n            for cy in range(ncy):\n                for cx in range(ncx):\n                    print(f\"chunk ({cz}, {cy}, {cx}) / ({ncz}, {ncy}, {ncx})\", end=\"\\r\")\n\n                    dat = prev_array[\n                        ...,\n                        cz * max_load : (cz + 1) * max_load,\n                        cy * max_load : (cy + 1) * max_load,\n                        cx * max_load : (cx + 1) * max_load,\n                    ]\n                    crop = [0 if x == 1 else x % 2 for x in dat.shape[-3:]]\n                    slicer = [slice(-1) if x else slice(None) for x in crop]\n                    dat = dat[(Ellipsis, *slicer)]\n                    pz, py, px = dat.shape[-3:]\n\n                    dat = dat.reshape(\n                        [\n                            nchannels,\n                            max(pz // 2, 1),\n                            min(pz, 2),\n                            max(py // 2, 1),\n                            min(py, 2),\n                            max(px // 2, 1),\n                            min(px, 2),\n                        ]\n                    )\n                    dat = dat.transpose([0, 1, 3, 5, 2, 4, 6])\n                    dat = dat.reshape(\n                        [\n                            nchannels,\n                            max(pz // 2, 1),\n                            max(py // 2, 1),\n                            max(px // 2, 1),\n                            -1,\n                        ]\n                    )\n                    dat = np.median(dat, -1)\n\n                    new_array[\n                        ...,\n                        cz * max_load // 2 : (cz + 1) * max_load // 2,\n                        cy * max_load // 2 : (cy + 1) * max_load // 2,\n                        cx * max_load // 2 : (cx + 1) * max_load // 2,\n                    ] = dat\n\n    print(\"\")\n    nblevel = level\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"z\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"y\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"median window 2x2x2\",\n            \"name\": \"\",\n        }\n    ]\n    multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    voxel_size = list(map(float, reversed(voxel_size)))\n    factor = [1] * 3\n    for n in range(nblevel):\n        shape = omz[str(n)].shape[-3:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # We made sure that the downsampling level is exactly 2\n        # However, once a dimension has size 1, we stop downsampling.\n        if n &gt; 0:\n            shape_prev = omz[str(n - 1)].shape[-3:]\n            if shape_prev[0] != shape[0]:\n                factor[0] *= 2\n            if shape_prev[1] != shape[1]:\n                factor[1] *= 2\n            if shape_prev[2] != shape[2]:\n                factor[2] *= 2\n\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0]\n                + [\n                    factor[0] * voxel_size[0],\n                    factor[1] * voxel_size[1],\n                    factor[2] * voxel_size[2],\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0]\n                + [\n                    (factor[0] - 1) * voxel_size[0] * 0.5,\n                    (factor[1] - 1) * voxel_size[1] * 0.5,\n                    (factor[2] - 1) * voxel_size[2] * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * 4, \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    if not nii:\n        print(\"done.\")\n        return\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    shape = shape[:3] + [1] + shape[3:]  # insert time dimension\n    affine = orientation_to_affine(orientation, *voxel_size)\n    if center:\n        affine = center_affine(affine, shape[:3])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"nz2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n    print(\"done.\")\n</code></pre>"},{"location":"api/utils/__init__/","title":"init","text":"<p>Various utilities.</p>"},{"location":"api/utils/j2k/","title":"j2k","text":"<p>Utilities for JPEG2000 files.</p>"},{"location":"api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K","title":"<code>WrappedJ2K</code>  <code>dataclass</code>","text":"<p>Array-like wrapper around a JPEG2000 object.</p> <p>A wrapper around the J2K object at any resolution level, and with virtual transposition of the axes into [C, H, W] order.</p> <p>The resulting object can be sliced, but each index must be a <code>slice</code> (dropping axes using integer indices or adding axes using <code>None</code> indices is forbidden).</p> <p>The point is to ensure that the zarr writer only loads chunk-sized data.</p>"},{"location":"api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K--parameters","title":"Parameters","text":"<p>j2k : glymur.Jp2k     The JPEG2000 object. level : int     Resolution level to map (highest resolution = 0). channel_first : bool     Return an array with shape (C, H, W) instead of (H, W, C)     when there is a channel dimension.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>@dataclass\nclass WrappedJ2K:\n    \"\"\"\n    Array-like wrapper around a JPEG2000 object.\n\n    A wrapper around the J2K object at any resolution level, and\n    with virtual transposition of the axes into [C, H, W] order.\n\n    The resulting object can be sliced, but each index must be a `slice`\n    (dropping axes using integer indices or adding axes using `None`\n    indices is forbidden).\n\n    The point is to ensure that the zarr writer only loads chunk-sized data.\n\n    Parameters\n    ----------\n    j2k : glymur.Jp2k\n        The JPEG2000 object.\n    level : int\n        Resolution level to map (highest resolution = 0).\n    channel_first : bool\n        Return an array with shape (C, H, W) instead of (H, W, C)\n        when there is a channel dimension.\n    \"\"\"\n\n    j2k: Jp2k\n    level: int = 0\n    channel_first: bool = True\n\n    @property\n    def shape(self) -&gt; tuple[int]:\n        \"\"\"Shape of the current level.\"\"\"\n        channel = list(self.j2k.shape[2:])\n        shape = [ceildiv(s, 2**self.level) for s in self.j2k.shape[:2]]\n        if self.channel_first:\n            shape = channel + shape\n        else:\n            shape += channel\n        return tuple(shape)\n\n    @property\n    def dtype(self) -&gt; np.dtype:\n        \"\"\"Data type of the wrapped image.\"\"\"\n        return self.j2k.dtype\n\n    def __getitem__(self, index: tuple[slice] | slice) -&gt; np.ndarray:\n        \"\"\"Multidimensional slicing of the wrapped array.\"\"\"\n        if not isinstance(index, tuple):\n            index = (index,)\n        if Ellipsis not in index:\n            index += (Ellipsis,)\n        if any(idx is None for idx in index):\n            raise TypeError(\"newaxis not supported\")\n\n        # substitute ellipses\n        new_index = []\n        has_seen_ellipsis = False\n        last_was_ellipsis = False\n        nb_ellipsis = max(0, self.j2k.ndim + 1 - len(index))\n        for idx in index:\n            if idx is Ellipsis:\n                if not has_seen_ellipsis:\n                    new_index += [slice(None)] * nb_ellipsis\n                elif not last_was_ellipsis:\n                    raise ValueError(\"Multiple ellipses should be contiguous\")\n                has_seen_ellipsis = True\n                last_was_ellipsis = True\n            elif not isinstance(idx, slice):\n                raise TypeError(\"Only slices are supported\")\n            elif idx.step not in (None, 1):\n                raise ValueError(\"Striding not supported\")\n            else:\n                last_was_ellipsis = False\n                new_index += [idx]\n        index = new_index\n\n        if self.channel_first:\n            *cidx, hidx, widx = index\n        else:\n            hidx, widx, *cidx = index\n        hstart, hstop = hidx.start or 0, hidx.stop or 0\n        wstart, wstop = widx.start or 0, widx.stop or 0\n\n        # convert to level 0 indices\n        hstart *= 2**self.level\n        hstop *= 2**self.level\n        wstart *= 2**self.level\n        wstop *= 2**self.level\n        hstop = min(hstop or self.j2k.shape[0], self.j2k.shape[0])\n        wstop = min(wstop or self.j2k.shape[1], self.j2k.shape[1])\n        area = (hstart, wstart, hstop, wstop)\n\n        data = self.j2k.read(rlevel=self.level, area=area)\n        if cidx:\n            data = data[:, :, cidx[0]]\n            if self.channel_first:\n                data = np.transpose(data, [2, 0, 1])\n        return data\n</code></pre>"},{"location":"api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K.dtype","title":"<code>dtype: np.dtype</code>  <code>property</code>","text":"<p>Data type of the wrapped image.</p>"},{"location":"api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K.shape","title":"<code>shape: tuple[int]</code>  <code>property</code>","text":"<p>Shape of the current level.</p>"},{"location":"api/utils/j2k/#linc_convert.utils.j2k.WrappedJ2K.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Multidimensional slicing of the wrapped array.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>def __getitem__(self, index: tuple[slice] | slice) -&gt; np.ndarray:\n    \"\"\"Multidimensional slicing of the wrapped array.\"\"\"\n    if not isinstance(index, tuple):\n        index = (index,)\n    if Ellipsis not in index:\n        index += (Ellipsis,)\n    if any(idx is None for idx in index):\n        raise TypeError(\"newaxis not supported\")\n\n    # substitute ellipses\n    new_index = []\n    has_seen_ellipsis = False\n    last_was_ellipsis = False\n    nb_ellipsis = max(0, self.j2k.ndim + 1 - len(index))\n    for idx in index:\n        if idx is Ellipsis:\n            if not has_seen_ellipsis:\n                new_index += [slice(None)] * nb_ellipsis\n            elif not last_was_ellipsis:\n                raise ValueError(\"Multiple ellipses should be contiguous\")\n            has_seen_ellipsis = True\n            last_was_ellipsis = True\n        elif not isinstance(idx, slice):\n            raise TypeError(\"Only slices are supported\")\n        elif idx.step not in (None, 1):\n            raise ValueError(\"Striding not supported\")\n        else:\n            last_was_ellipsis = False\n            new_index += [idx]\n    index = new_index\n\n    if self.channel_first:\n        *cidx, hidx, widx = index\n    else:\n        hidx, widx, *cidx = index\n    hstart, hstop = hidx.start or 0, hidx.stop or 0\n    wstart, wstop = widx.start or 0, widx.stop or 0\n\n    # convert to level 0 indices\n    hstart *= 2**self.level\n    hstop *= 2**self.level\n    wstart *= 2**self.level\n    wstop *= 2**self.level\n    hstop = min(hstop or self.j2k.shape[0], self.j2k.shape[0])\n    wstop = min(wstop or self.j2k.shape[1], self.j2k.shape[1])\n    area = (hstart, wstart, hstop, wstop)\n\n    data = self.j2k.read(rlevel=self.level, area=area)\n    if cidx:\n        data = data[:, :, cidx[0]]\n        if self.channel_first:\n            data = np.transpose(data, [2, 0, 1])\n    return data\n</code></pre>"},{"location":"api/utils/j2k/#linc_convert.utils.j2k.get_pixelsize","title":"<code>get_pixelsize(j2k)</code>","text":"<p>Read pixelsize from the JPEG2000 file.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>def get_pixelsize(j2k: Jp2k) -&gt; tuple[float, float]:\n    \"\"\"Read pixelsize from the JPEG2000 file.\"\"\"\n    # Adobe XMP metadata\n    # https://en.wikipedia.org/wiki/Extensible_Metadata_Platform\n    XMP_UUID = \"BE7ACFCB97A942E89C71999491E3AFAC\"\n    TAG_Images = \"{http://ns.adobe.com/xap/1.0/}Images\"\n    Tag_Desc = \"{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Description\"\n    Tag_PixelWidth = \"{http://ns.adobe.com/xap/1.0/}PixelWidth\"\n    Tag_PixelHeight = \"{http://ns.adobe.com/xap/1.0/}PixelHeight\"\n\n    vxw = vxh = 1.0\n    for box in j2k.box:\n        if getattr(box, \"uuid\", None) == uuid.UUID(XMP_UUID):\n            try:\n                images = list(box.data.iter(TAG_Images))[0]\n                desc = list(images.iter(Tag_Desc))[0]\n                vxw = float(desc.attrib[Tag_PixelWidth])\n                vxh = float(desc.attrib[Tag_PixelHeight])\n            except Exception:\n                pass\n    return vxw, vxh\n</code></pre>"},{"location":"api/utils/math/","title":"math","text":"<p>Math utilities.</p>"},{"location":"api/utils/math/#linc_convert.utils.math.ceildiv","title":"<code>ceildiv(x, y)</code>","text":"<p>Ceil of ratio of two numbers.</p> Source code in <code>linc_convert/utils/math.py</code> <pre><code>def ceildiv(x: Number, y: Number) -&gt; int:\n    \"\"\"Ceil of ratio of two numbers.\"\"\"\n    return int(math.ceil(x / y))\n</code></pre>"},{"location":"api/utils/math/#linc_convert.utils.math.floordiv","title":"<code>floordiv(x, y)</code>","text":"<p>Floor of ratio of two numbers.</p> Source code in <code>linc_convert/utils/math.py</code> <pre><code>def floordiv(x: Number, y: Number) -&gt; int:\n    \"\"\"Floor of ratio of two numbers.\"\"\"\n    return int(math.floor(x / y))\n</code></pre>"},{"location":"api/utils/orientation/","title":"orientation","text":"<p>Orientation of an array of voxels with respect to world space.</p>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.center_affine","title":"<code>center_affine(affine, shape)</code>","text":"<p>Ensure that the center of the field-of-view has world coordinate (0,0,0).</p> <p>The input affine is NOT modified in-place</p>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.center_affine--parameters","title":"Parameters","text":"<p>affine : array     Orientation affine matrix shape : list[int]     Shape of the array of voxels</p>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.center_affine--returns","title":"Returns","text":"<p>affine : array     Modified affine matrix.</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def center_affine(affine: np.ndarray, shape: list[int]) -&gt; np.ndarray:\n    \"\"\"\n    Ensure that the center of the field-of-view has world coordinate (0,0,0).\n\n    !!! note \"The input affine is NOT modified in-place\"\n\n    Parameters\n    ----------\n    affine : array\n        Orientation affine matrix\n    shape : list[int]\n        Shape of the array of voxels\n\n    Returns\n    -------\n    affine : array\n        Modified affine matrix.\n    \"\"\"\n    if len(shape) == 2:\n        shape = [*shape, 1]\n    shape = np.asarray(shape)\n    affine = np.copy(affine)\n    affine[:3, -1] = -0.5 * affine[:3, :3] @ (shape - 1)\n    return affine\n</code></pre>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.orientation_ensure_3d","title":"<code>orientation_ensure_3d(orientation)</code>","text":"<p>Convert an ND orientation string to a 3D orientation string.</p>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.orientation_ensure_3d--parameters","title":"Parameters","text":"<p>orientation : str     A 2D or 3D orientation string, such as <code>\"RA\"</code> or <code>\"RAS\"</code>.</p>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.orientation_ensure_3d--returns","title":"Returns","text":"<p>orientation : str     A 3D orientation string compatible with the input orientaition</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def orientation_ensure_3d(orientation: str) -&gt; str:\n    \"\"\"\n    Convert an ND orientation string to a 3D orientation string.\n\n    Parameters\n    ----------\n    orientation : str\n        A 2D or 3D orientation string, such as `\"RA\"` or `\"RAS\"`.\n\n    Returns\n    -------\n    orientation : str\n        A 3D orientation string compatible with the input orientaition\n    \"\"\"\n    orientation = {\n        \"coronal\": \"LI\",\n        \"axial\": \"LP\",\n        \"sagittal\": \"PI\",\n    }.get(orientation.lower(), orientation).upper()\n    if len(orientation) == 2:\n        if \"L\" not in orientation and \"R\" not in orientation:\n            orientation += \"R\"\n        if \"P\" not in orientation and \"A\" not in orientation:\n            orientation += \"A\"\n        if \"I\" not in orientation and \"S\" not in orientation:\n            orientation += \"S\"\n    return orientation\n</code></pre>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.orientation_to_affine","title":"<code>orientation_to_affine(orientation, vxw=1, vxh=1, vxd=1)</code>","text":"<p>Build an affine matrix from an orientation string and voxel size.</p>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.orientation_to_affine--parameters","title":"Parameters","text":"<p>orientation : str     Orientation string vxw : float     Width voxel size vxh : float     Height voxel size vxd : float     Depth voxel size</p>"},{"location":"api/utils/orientation/#linc_convert.utils.orientation.orientation_to_affine--returns","title":"Returns","text":"<p>affine : (4, 4) array     Affine orientation matrix</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def orientation_to_affine(\n    orientation: str, vxw: float = 1, vxh: float = 1, vxd: float = 1\n) -&gt; np.ndarray:\n    \"\"\"\n    Build an affine matrix from an orientation string and voxel size.\n\n    Parameters\n    ----------\n    orientation : str\n        Orientation string\n    vxw : float\n        Width voxel size\n    vxh : float\n        Height voxel size\n    vxd : float\n        Depth voxel size\n\n    Returns\n    -------\n    affine : (4, 4) array\n        Affine orientation matrix\n    \"\"\"\n    orientation = orientation_ensure_3d(orientation)\n    affine = np.zeros([4, 4])\n    vx = np.asarray([vxw, vxh, vxd])\n    for i in range(3):\n        letter = orientation[i]\n        sign = -1 if letter in \"LPI\" else 1\n        letter = {\"L\": \"R\", \"P\": \"A\", \"I\": \"S\"}.get(letter, letter)\n        index = list(\"RAS\").index(letter)\n        affine[index, i] = sign * vx[i]\n    return affine\n</code></pre>"},{"location":"api/utils/zarr/","title":"zarr","text":"<p>Zarr utilities.</p>"},{"location":"api/utils/zarr/#linc_convert.utils.zarr.make_compressor","title":"<code>make_compressor(name, **prm)</code>","text":"<p>Build compressor object from name and options.</p> Source code in <code>linc_convert/utils/zarr.py</code> <pre><code>def make_compressor(name: str, **prm: dict) -&gt; numcodecs.abc.Codec:\n    \"\"\"Build compressor object from name and options.\"\"\"\n    # TODO: we should use `numcodecs.get_codec` instead`\n    if not isinstance(name, str):\n        return name\n    name = name.lower()\n    if name == \"blosc\":\n        Compressor = numcodecs.Blosc\n    elif name == \"zlib\":\n        Compressor = numcodecs.Zlib\n    else:\n        raise ValueError(\"Unknown compressor\", name)\n    return Compressor(**prm)\n</code></pre>"},{"location":"docs/api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cli</li> <li>modalities<ul> <li>df<ul> <li>cli</li> <li>multi_slice</li> <li>single_slice</li> </ul> </li> <li>lsm<ul> <li>cli</li> <li>mosaic</li> </ul> </li> </ul> </li> <li>utils<ul> <li>j2k</li> <li>math</li> <li>orientation</li> <li>zarr</li> </ul> </li> </ul>"},{"location":"docs/api/cli/","title":"Cli","text":"<p>Root command line entry point.</p>"},{"location":"docs/api/modalities/__init__/","title":"init","text":"<p>Converters for all imaging modalities.</p>"},{"location":"docs/api/modalities/df/__init__/","title":"init","text":"<p>Dark Field microscopy converters.</p>"},{"location":"docs/api/modalities/df/cli/","title":"Cli","text":"<p>Entry-points for Dark Field microscopy converter.</p>"},{"location":"docs/api/modalities/df/multi_slice/","title":"Multi slice","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a OME-ZARR pyramid.</p> <p>We do not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"docs/api/modalities/df/multi_slice/#modalities.df.multi_slice.convert","title":"<code>convert(inp, out=None, *, chunk=4096, compressor='blosc', compressor_opt='{}', max_load=16384, nii=False, orientation='coronal', center=True, thickness=None)</code>","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p> <p>This command converts a batch of slices and stacks them together into a single 3D Zarr.</p>"},{"location":"docs/api/modalities/df/multi_slice/#modalities.df.multi_slice.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and   indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and   indicates the anatomical meaning of the bottom of the jp2 image,</li> <li>the third letter corresponds to the slice dimension and   indicates the anatomical meaninff of the end of the stack.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"docs/api/modalities/df/multi_slice/#modalities.df.multi_slice.convert--parameters","title":"Parameters","text":"<p>inp     Path to the input slices out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\" orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center thickness     Slice thickness Source code in <code>linc_convert/modalities/df/multi_slice.py</code> <pre><code>@ms.default\ndef convert(\n    inp: list[str],\n    out: str | None = None,\n    *,\n    chunk: int = 4096,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 16384,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.\n\n    It does not recompute the image pyramid but instead reuse the\n    JPEG2000 levels (obtained by wavelet transform).\n\n    This command converts a batch of slices and stacks them together\n    into a single 3D Zarr.\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n      indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n      indicates the anatomical meaning of the _bottom_ of the jp2 image,\n    * the third letter corresponds to the slice dimension and\n      indicates the anatomical meaninff of the _end_ of the stack.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the input slices\n    out\n        Path to the output Zarr directory [&lt;dirname(INP)&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\"\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    thickness\n        Slice thickness\n    \"\"\"\n    # Default output path\n    if not out:\n        out = os.path.splitext(inp[0])[0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    nblevel, has_channel, dtype_jp2 = float(\"inf\"), float(\"inf\"), \"\"\n\n    # Compute output shape\n    new_height, new_width = 0, 0\n    for inp1 in inp:\n        jp2 = glymur.Jp2k(inp1)\n        nblevel = min(nblevel, jp2.codestream.segment[2].num_res)\n        has_channel = min(has_channel, jp2.ndim - 2)\n        dtype_jp2 = np.dtype(jp2.dtype).str\n        if jp2.shape[0] &gt; new_height:\n            new_height = jp2.shape[0]\n        if jp2.shape[1] &gt; new_width:\n            new_width = jp2.shape[1]\n    new_size = (new_height, new_width)\n    if has_channel:\n        new_size += (3.0,)\n    print(len(inp), new_size, nblevel, has_channel)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": list(new_size[2:]) + [1] + [chunk, chunk],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": dtype_jp2,\n        \"fill_value\": 0,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n    print(opt)\n    print(new_size)\n    # Write each level\n    for level in range(nblevel):\n        shape = [ceildiv(s, 2**level) for s in new_size[:2]]\n        shape = [new_size[2]] + [len(inp)] + shape\n\n        omz.create_dataset(f\"{level}\", shape=shape, **opt)\n        array = omz[f\"{level}\"]\n\n        # Write each slice\n        for idx, inp1 in enumerate(inp):\n            j2k = glymur.Jp2k(inp1)\n            vxw, vxh = get_pixelsize(j2k)\n            subdat = WrappedJ2K(j2k, level=level)\n            subdat_size = subdat.shape\n            print(\n                \"Convert level\",\n                level,\n                \"with shape\",\n                shape,\n                \"for slice\",\n                idx,\n                \"with size\",\n                subdat_size,\n            )\n\n            # offset while attaching\n            x = floordiv(shape[-2] - subdat_size[-2], 2)\n            y = floordiv(shape[-1] - subdat_size[-1], 2)\n\n            if max_load is None or (shape[-2] &lt; max_load and shape[-1] &lt; max_load):\n                array[..., idx, x : x + subdat_size[1], y : y + subdat_size[2]] = (\n                    subdat[...]\n                )\n\n            else:\n                ni = ceildiv(shape[-2], max_load)\n                nj = ceildiv(shape[-1], max_load)\n\n                for i in range(ni):\n                    for j in range(nj):\n                        print(f\"\\r{i+1}/{ni}, {j+1}/{nj}\", end=\" \")\n                        start_x, end_x = (\n                            i * max_load,\n                            min((i + 1) * max_load, shape[-2]),\n                        )\n                        start_y, end_y = (\n                            j * max_load,\n                            min((j + 1) * max_load, shape[-1]),\n                        )\n                        if end_x &lt;= x or end_y &lt;= y:\n                            continue\n\n                        if start_x &gt;= subdat_size[-2] or start_y &gt;= subdat_size[-1]:\n                            continue\n\n                        array[\n                            ...,\n                            idx,\n                            x + start_x : x + min(end_x, subdat_size[-2]),\n                            y + start_y : y + min(end_y, subdat_size[-1]),\n                        ] = subdat[\n                            ...,\n                            start_x : min((i + 1) * max_load, subdat_size[-2]),\n                            start_y : min((j + 1) * max_load, subdat_size[-1]),\n                        ]\n                print(\"\")\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"z\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"y\", \"type\": \"distance\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"jpeg2000\",\n            \"name\": \"\",\n        }\n    ]\n    if has_channel:\n        multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    for n in range(nblevel):\n        shape0 = omz[\"0\"].shape[-2:]\n        shape = omz[str(n)].shape[-2:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # I assume that wavelet transforms end up aligning voxel edges\n        # across levels, so the effective scaling is the shape ratio,\n        # and there is a half voxel shift wrt to the \"center of first voxel\"\n        # frame\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0] * has_channel\n                + [\n                    1.0,\n                    (shape0[0] / shape[0]) * vxh,\n                    (shape0[1] / shape[1]) * vxw,\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0] * has_channel\n                + [\n                    0.0,\n                    (shape0[0] / shape[0] - 1) * vxh * 0.5,\n                    (shape0[1] / shape[1] - 1) * vxw * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * (3 + has_channel), \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    if has_channel:\n        shape = shape[:3] + [1] + shape[3:]\n    affine = orientation_to_affine(orientation, vxw, vxh, thickness or 1)\n    if center:\n        affine = center_affine(affine, shape[:2])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"n+2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n\n    # Write sidecar .json file\n    json_name = os.path.splitext(out)[0]\n    json_name += \".json\"\n    dic = {}\n    dic[\"PixelSize\"] = json.dumps([vxw, vxh])\n    dic[\"PixelSizeUnits\"] = \"um\"\n    dic[\"SliceThickness\"] = 1.2\n    dic[\"SliceThicknessUnits\"] = \"mm\"\n    dic[\"SampleStaining\"] = \"LY\"\n\n    with open(json_name, \"w\") as outfile:\n        json.dump(dic, outfile)\n        outfile.write(\"\\n\")\n\n    print(\"done.\")\n</code></pre>"},{"location":"docs/api/modalities/df/single_slice/","title":"Single slice","text":"<p>Converts JPEG2000 files generated by MBF-Neurolucida into a OME-ZARR pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"docs/api/modalities/df/single_slice/#modalities.df.single_slice.convert","title":"<code>convert(inp, out=None, *, chunk=1024, compressor='blosc', compressor_opt='{}', max_load=16384, nii=False, orientation='coronal', center=True, thickness=None)</code>","text":"<p>Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.</p> <p>It does not recompute the image pyramid but instead reuse the JPEG2000 levels (obtained by wavelet transform).</p>"},{"location":"docs/api/modalities/df/single_slice/#modalities.df.single_slice.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and     indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and     indicates the anatomical meaning of the bottom of the jp2 image.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"docs/api/modalities/df/single_slice/#modalities.df.single_slice.convert--parameters","title":"Parameters","text":"<p>inp     Path to the input JP2 file out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\" orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center thickness     Slice thickness Source code in <code>linc_convert/modalities/df/single_slice.py</code> <pre><code>@ss.default\ndef convert(\n    inp: str,\n    out: str | None = None,\n    *,\n    chunk: int = 1024,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 16384,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Convert JPEG2000 files generated by MBF-Neurolucida into a Zarr pyramid.\n\n    It does not recompute the image pyramid but instead reuse the JPEG2000\n    levels (obtained by wavelet transform).\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n        indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n        indicates the anatomical meaning of the _bottom_ of the jp2 image.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the input JP2 file\n    out\n        Path to the output Zarr directory [&lt;INP&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\"\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    thickness\n        Slice thickness\n    \"\"\"\n    if not out:\n        out = os.path.splitext(inp)[0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    j2k = glymur.Jp2k(inp)\n    vxw, vxh = get_pixelsize(j2k)\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": list(j2k.shape[2:]) + [chunk, chunk],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": np.dtype(j2k.dtype).str,\n        \"fill_value\": None,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n\n    # Write each level\n    nblevel = j2k.codestream.segment[2].num_res\n    has_channel = j2k.ndim - 2\n    for level in range(nblevel):\n        subdat = WrappedJ2K(j2k, level=level)\n        shape = subdat.shape\n        print(\"Convert level\", level, \"with shape\", shape)\n        omz.create_dataset(str(level), shape=shape, **opt)\n        array = omz[str(level)]\n        if max_load is None or (shape[-2] &lt; max_load and shape[-1] &lt; max_load):\n            array[...] = subdat[...]\n        else:\n            ni = ceildiv(shape[-2], max_load)\n            nj = ceildiv(shape[-1], max_load)\n            for i in range(ni):\n                for j in range(nj):\n                    print(f\"\\r{i+1}/{ni}, {j+1}/{nj}\", end=\"\")\n                    array[\n                        ...,\n                        i * max_load : min((i + 1) * max_load, shape[-2]),\n                        j * max_load : min((j + 1) * max_load, shape[-1]),\n                    ] = subdat[\n                        ...,\n                        i * max_load : min((i + 1) * max_load, shape[-2]),\n                        j * max_load : min((j + 1) * max_load, shape[-1]),\n                    ]\n            print(\"\")\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"y\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"jpeg2000\",\n            \"name\": \"\",\n        }\n    ]\n    if has_channel:\n        multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    for n in range(nblevel):\n        shape0 = omz[\"0\"].shape[-2:]\n        shape = omz[str(n)].shape[-2:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # I assume that wavelet transforms end up aligning voxel edges\n        # across levels, so the effective scaling is the shape ratio,\n        # and there is a half voxel shift wrt to the \"center of first voxel\"\n        # frame\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0] * has_channel\n                + [\n                    (shape0[0] / shape[0]) * vxh,\n                    (shape0[1] / shape[1]) * vxw,\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0] * has_channel\n                + [\n                    (shape0[0] / shape[0] - 1) * vxh * 0.5,\n                    (shape0[1] / shape[1] - 1) * vxw * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * (2 + has_channel), \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    if not nii:\n        print(\"done.\")\n        return\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    if has_channel:\n        shape = shape[:2] + [1, 1] + shape[2:]\n    affine = orientation_to_affine(orientation, vxw, vxh, thickness or 1)\n    if center:\n        affine = center_affine(affine, shape[:2])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"n+2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n    print(\"done.\")\n</code></pre>"},{"location":"docs/api/modalities/lsm/__init__/","title":"init","text":"<p>Light Sheet Microscopy converters.</p>"},{"location":"docs/api/modalities/lsm/cli/","title":"Cli","text":"<p>Entry-points for Dark Field microscopy converter.</p>"},{"location":"docs/api/modalities/lsm/mosaic/","title":"Mosaic","text":"<p>Convert a collection of tiff files generated by the LSM pipeline into a Zarr.</p> <p>Example input files can be found at https://lincbrain.org/dandiset/000004/0.240319.1924/files?location=derivatives%2F</p>"},{"location":"docs/api/modalities/lsm/mosaic/#modalities.lsm.mosaic.convert","title":"<code>convert(inp, out=None, *, chunk=128, compressor='blosc', compressor_opt='{}', max_load=512, nii=False, orientation='coronal', center=True, thickness=None, voxel_size=(1, 1, 1))</code>","text":"<p>Convert a collection of tiff files generated by the LSM pipeline into ZARR.</p>"},{"location":"docs/api/modalities/lsm/mosaic/#modalities.lsm.mosaic.convert--orientation","title":"Orientation","text":"<p>The anatomical orientation of the slice is given in terms of RAS axes.</p> <p>It is a combination of two letters from the set <code>{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}</code>, where</p> <ul> <li>the first letter corresponds to the horizontal dimension and     indicates the anatomical meaning of the right of the jp2 image,</li> <li>the second letter corresponds to the vertical dimension and     indicates the anatomical meaning of the bottom of the jp2 image.</li> </ul> <p>We also provide the aliases</p> <ul> <li><code>\"coronal\"</code> == <code>\"LI\"</code></li> <li><code>\"axial\"</code> == <code>\"LP\"</code></li> <li><code>\"sagittal\"</code> == <code>\"PI\"</code></li> </ul> <p>The orientation flag is only useful when converting to nifti-zarr.</p>"},{"location":"docs/api/modalities/lsm/mosaic/#modalities.lsm.mosaic.convert--parameters","title":"Parameters","text":"<p>inp     Path to the root directory, which contains a collection of     subfolders named <code>*_z{:02d}_y{:02d}*</code>, each containing a     collection of files named <code>*_plane{:03d}_c{:d}.tiff</code>. out     Path to the output Zarr directory [.ome.zarr] chunk     Output chunk size compressor : {blosc, zlib, raw}     Compression method compressor_opt     Compression options max_load     Maximum input chunk size when building pyramid nii     Convert to nifti-zarr. True if path ends in \".nii.zarr\". orientation     Orientation of the slice center     Set RAS[0, 0, 0] at FOV center voxel_size     Voxel size along the X, Y and Z dimension, in micron. Source code in <code>linc_convert/modalities/lsm/mosaic.py</code> <pre><code>@mosaic.default\ndef convert(\n    inp: str,\n    out: str = None,\n    *,\n    chunk: int = 128,\n    compressor: str = \"blosc\",\n    compressor_opt: str = \"{}\",\n    max_load: int = 512,\n    nii: bool = False,\n    orientation: str = \"coronal\",\n    center: bool = True,\n    thickness: float | None = None,\n    voxel_size: list[float] = (1, 1, 1),\n) -&gt; None:\n    \"\"\"\n    Convert a collection of tiff files generated by the LSM pipeline into ZARR.\n\n    Orientation\n    -----------\n    The anatomical orientation of the slice is given in terms of RAS axes.\n\n    It is a combination of two letters from the set\n    `{\"L\", \"R\", \"A\", \"P\", \"I\", \"S\"}`, where\n\n    * the first letter corresponds to the horizontal dimension and\n        indicates the anatomical meaning of the _right_ of the jp2 image,\n    * the second letter corresponds to the vertical dimension and\n        indicates the anatomical meaning of the _bottom_ of the jp2 image.\n\n    We also provide the aliases\n\n    * `\"coronal\"` == `\"LI\"`\n    * `\"axial\"` == `\"LP\"`\n    * `\"sagittal\"` == `\"PI\"`\n\n    The orientation flag is only useful when converting to nifti-zarr.\n\n    Parameters\n    ----------\n    inp\n        Path to the root directory, which contains a collection of\n        subfolders named `*_z{:02d}_y{:02d}*`, each containing a\n        collection of files named `*_plane{:03d}_c{:d}.tiff`.\n    out\n        Path to the output Zarr directory [&lt;INP&gt;.ome.zarr]\n    chunk\n        Output chunk size\n    compressor : {blosc, zlib, raw}\n        Compression method\n    compressor_opt\n        Compression options\n    max_load\n        Maximum input chunk size when building pyramid\n    nii\n        Convert to nifti-zarr. True if path ends in \".nii.zarr\".\n    orientation\n        Orientation of the slice\n    center\n        Set RAS[0, 0, 0] at FOV center\n    voxel_size\n        Voxel size along the X, Y and Z dimension, in micron.\n    \"\"\"\n    if isinstance(compressor_opt, str):\n        compressor_opt = ast.literal_eval(compressor_opt)\n\n    if max_load % 2:\n        max_load += 1\n\n    CHUNK_PATTERN = re.compile(\n        r\"^(?P&lt;prefix&gt;\\w*)\" r\"_z(?P&lt;z&gt;[0-9]+)\" r\"_y(?P&lt;y&gt;[0-9]+)\" r\"(?P&lt;suffix&gt;\\w*)$\"\n    )\n\n    all_chunks_dirnames = list(sorted(glob(os.path.join(inp, \"*_z*_y*\"))))\n    all_chunks_info = dict(\n        dirname=[],\n        prefix=[],\n        suffix=[],\n        z=[],\n        y=[],\n        planes=[\n            dict(\n                fname=[],\n                z=[],\n                c=[],\n                yx_shape=[],\n            )\n            for _ in range(len(all_chunks_dirnames))\n        ],\n    )\n\n    # parse all directory names\n    for dirname in all_chunks_dirnames:\n        parsed = CHUNK_PATTERN.fullmatch(os.path.basename(dirname))\n        all_chunks_info[\"dirname\"].append(dirname)\n        all_chunks_info[\"prefix\"].append(parsed.group(\"prefix\"))\n        all_chunks_info[\"suffix\"].append(parsed.group(\"suffix\"))\n        all_chunks_info[\"z\"].append(int(parsed.group(\"z\")))\n        all_chunks_info[\"y\"].append(int(parsed.group(\"y\")))\n\n    # default output name\n    if not out:\n        out = all_chunks_info[\"prefix\"][0] + all_chunks_info[\"suffix\"][0]\n        out += \".nii.zarr\" if nii else \".ome.zarr\"\n    nii = nii or out.endswith(\".nii.zarr\")\n\n    # parse all individual file names\n    nchunkz = max(all_chunks_info[\"z\"])\n    nchunky = max(all_chunks_info[\"y\"])\n    allshapes = [[(0, 0, 0) for _ in range(nchunky)] for _ in range(nchunkz)]\n    nchannels = 0\n    dtype = None\n    for zchunk in range(nchunkz):\n        for ychunk in range(nchunky):\n            for i in range(len(all_chunks_info[\"dirname\"])):\n                if (\n                    all_chunks_info[\"z\"][i] == zchunk + 1\n                    and all_chunks_info[\"y\"][i] == ychunk + 1\n                ):\n                    break\n            dirname = all_chunks_info[\"dirname\"][i]\n            planes_filenames = list(sorted(glob(os.path.join(dirname, \"*.tiff\"))))\n\n            PLANE_PATTERN = re.compile(\n                os.path.basename(dirname) + r\"_plane(?P&lt;z&gt;[0-9]+)\"\n                r\"_c(?P&lt;c&gt;[0-9]+)\"\n                r\".tiff$\"\n            )\n\n            for fname in planes_filenames:\n                parsed = PLANE_PATTERN.fullmatch(os.path.basename(fname))\n                all_chunks_info[\"planes\"][i][\"fname\"] += [fname]\n                all_chunks_info[\"planes\"][i][\"z\"] += [int(parsed.group(\"z\"))]\n                all_chunks_info[\"planes\"][i][\"c\"] += [int(parsed.group(\"c\"))]\n\n                f = TiffFile(fname)\n                dtype = f.pages[0].dtype\n                yx_shape = f.pages[0].shape\n                all_chunks_info[\"planes\"][i][\"yx_shape\"].append(yx_shape)\n\n            nplanes = max(all_chunks_info[\"planes\"][i][\"z\"])\n            nchannels = max(nchannels, max(all_chunks_info[\"planes\"][i][\"c\"]))\n\n            yx_shape = set(all_chunks_info[\"planes\"][i][\"yx_shape\"])\n            if not len(yx_shape) == 1:\n                raise ValueError(\"Incompatible chunk shapes\")\n            yx_shape = list(yx_shape)[0]\n            allshapes[zchunk][ychunk] = (nplanes, *yx_shape)\n\n    # check that all chink shapes are compatible\n    for zchunk in range(nchunkz):\n        if len(set(shape[1] for shape in allshapes[zchunk])) != 1:\n            raise ValueError(\"Incompatible Y shapes\")\n    for ychunk in range(nchunky):\n        if len(set(shape[ychunk][0] for shape in allshapes)) != 1:\n            raise ValueError(\"Incompatible Z shapes\")\n    if len(set(shape[2] for subshapes in allshapes for shape in subshapes)) != 1:\n        raise ValueError(\"Incompatible X shapes\")\n\n    # compute full shape\n    fullshape = [0, 0, 0]\n    fullshape[0] = sum(shape[0][0] for shape in allshapes)\n    fullshape[1] = sum(shape[1] for shape in allshapes[0])\n    fullshape[2] = allshapes[0][0][2]\n\n    # Prepare Zarr group\n    omz = zarr.storage.DirectoryStore(out)\n    omz = zarr.group(store=omz, overwrite=True)\n\n    # Prepare chunking options\n    opt = {\n        \"chunks\": [nchannels] + [chunk] * 3,\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": np.dtype(dtype).str,\n        \"fill_value\": None,\n        \"compressor\": make_compressor(compressor, **compressor_opt),\n    }\n\n    # write first level\n    omz.create_dataset(\"0\", shape=[nchannels, *fullshape], **opt)\n    array = omz[\"0\"]\n    print(\"Write level 0 with shape\", [nchannels, *fullshape])\n    for i, dirname in enumerate(all_chunks_info[\"dirname\"]):\n        chunkz = all_chunks_info[\"z\"][i] - 1\n        chunky = all_chunks_info[\"y\"][i] - 1\n        planes = all_chunks_info[\"planes\"][i]\n        for j, fname in enumerate(planes[\"fname\"]):\n            subz = planes[\"z\"][j] - 1\n            subc = planes[\"c\"][j] - 1\n            yx_shape = planes[\"yx_shape\"][j]\n\n            zstart = sum(shape[0][0] for shape in allshapes[:chunkz])\n            ystart = sum(\n                shape[1] for subshapes in allshapes for shape in subshapes[:chunky]\n            )\n            print(\n                f\"Write plane \"\n                f\"({subc}, {zstart + subz}, {ystart}:{ystart + yx_shape[0]})\",\n                end=\"\\r\",\n            )\n            slicer = (\n                subc,\n                zstart + subz,\n                slice(ystart, ystart + yx_shape[0]),\n                slice(None),\n            )\n\n            f = TiffFile(fname)\n            array[slicer] = f.asarray()\n    print(\"\")\n\n    # build pyramid using median windows\n    level = 0\n    while any(x &gt; 1 for x in omz[str(level)].shape[-3:]):\n        prev_array = omz[str(level)]\n        prev_shape = prev_array.shape[-3:]\n        level += 1\n\n        new_shape = list(map(lambda x: max(1, x // 2), prev_shape))\n        if all(x &lt; chunk for x in new_shape):\n            break\n        print(\"Compute level\", level, \"with shape\", new_shape)\n        omz.create_dataset(str(level), shape=[nchannels, *new_shape], **opt)\n        new_array = omz[str(level)]\n\n        nz, ny, nx = prev_array.shape[-3:]\n        ncz = ceildiv(nz, max_load)\n        ncy = ceildiv(ny, max_load)\n        ncx = ceildiv(nx, max_load)\n\n        for cz in range(ncz):\n            for cy in range(ncy):\n                for cx in range(ncx):\n                    print(f\"chunk ({cz}, {cy}, {cx}) / ({ncz}, {ncy}, {ncx})\", end=\"\\r\")\n\n                    dat = prev_array[\n                        ...,\n                        cz * max_load : (cz + 1) * max_load,\n                        cy * max_load : (cy + 1) * max_load,\n                        cx * max_load : (cx + 1) * max_load,\n                    ]\n                    crop = [0 if x == 1 else x % 2 for x in dat.shape[-3:]]\n                    slicer = [slice(-1) if x else slice(None) for x in crop]\n                    dat = dat[(Ellipsis, *slicer)]\n                    pz, py, px = dat.shape[-3:]\n\n                    dat = dat.reshape(\n                        [\n                            nchannels,\n                            max(pz // 2, 1),\n                            min(pz, 2),\n                            max(py // 2, 1),\n                            min(py, 2),\n                            max(px // 2, 1),\n                            min(px, 2),\n                        ]\n                    )\n                    dat = dat.transpose([0, 1, 3, 5, 2, 4, 6])\n                    dat = dat.reshape(\n                        [\n                            nchannels,\n                            max(pz // 2, 1),\n                            max(py // 2, 1),\n                            max(px // 2, 1),\n                            -1,\n                        ]\n                    )\n                    dat = np.median(dat, -1)\n\n                    new_array[\n                        ...,\n                        cz * max_load // 2 : (cz + 1) * max_load // 2,\n                        cy * max_load // 2 : (cy + 1) * max_load // 2,\n                        cx * max_load // 2 : (cx + 1) * max_load // 2,\n                    ] = dat\n\n    print(\"\")\n    nblevel = level\n\n    # Write OME-Zarr multiscale metadata\n    print(\"Write metadata\")\n    multiscales = [\n        {\n            \"version\": \"0.4\",\n            \"axes\": [\n                {\"name\": \"z\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"y\", \"type\": \"space\", \"unit\": \"micrometer\"},\n                {\"name\": \"x\", \"type\": \"space\", \"unit\": \"micrometer\"},\n            ],\n            \"datasets\": [],\n            \"type\": \"median window 2x2x2\",\n            \"name\": \"\",\n        }\n    ]\n    multiscales[0][\"axes\"].insert(0, {\"name\": \"c\", \"type\": \"channel\"})\n\n    voxel_size = list(map(float, reversed(voxel_size)))\n    factor = [1] * 3\n    for n in range(nblevel):\n        shape = omz[str(n)].shape[-3:]\n        multiscales[0][\"datasets\"].append({})\n        level = multiscales[0][\"datasets\"][-1]\n        level[\"path\"] = str(n)\n\n        # We made sure that the downsampling level is exactly 2\n        # However, once a dimension has size 1, we stop downsampling.\n        if n &gt; 0:\n            shape_prev = omz[str(n - 1)].shape[-3:]\n            if shape_prev[0] != shape[0]:\n                factor[0] *= 2\n            if shape_prev[1] != shape[1]:\n                factor[1] *= 2\n            if shape_prev[2] != shape[2]:\n                factor[2] *= 2\n\n        level[\"coordinateTransformations\"] = [\n            {\n                \"type\": \"scale\",\n                \"scale\": [1.0]\n                + [\n                    factor[0] * voxel_size[0],\n                    factor[1] * voxel_size[1],\n                    factor[2] * voxel_size[2],\n                ],\n            },\n            {\n                \"type\": \"translation\",\n                \"translation\": [0.0]\n                + [\n                    (factor[0] - 1) * voxel_size[0] * 0.5,\n                    (factor[1] - 1) * voxel_size[1] * 0.5,\n                    (factor[2] - 1) * voxel_size[2] * 0.5,\n                ],\n            },\n        ]\n    multiscales[0][\"coordinateTransformations\"] = [\n        {\"scale\": [1.0] * 4, \"type\": \"scale\"}\n    ]\n    omz.attrs[\"multiscales\"] = multiscales\n\n    if not nii:\n        print(\"done.\")\n        return\n\n    # Write NIfTI-Zarr header\n    # NOTE: we use nifti2 because dimensions typically do not fit in a short\n    # TODO: we do not write the json zattrs, but it should be added in\n    #       once the nifti-zarr package is released\n    shape = list(reversed(omz[\"0\"].shape))\n    shape = shape[:3] + [1] + shape[3:]  # insert time dimension\n    affine = orientation_to_affine(orientation, *voxel_size)\n    if center:\n        affine = center_affine(affine, shape[:3])\n    header = nib.Nifti2Header()\n    header.set_data_shape(shape)\n    header.set_data_dtype(omz[\"0\"].dtype)\n    header.set_qform(affine)\n    header.set_sform(affine)\n    header.set_xyzt_units(nib.nifti1.unit_codes.code[\"micron\"])\n    header.structarr[\"magic\"] = b\"nz2\\0\"\n    header = np.frombuffer(header.structarr.tobytes(), dtype=\"u1\")\n    opt = {\n        \"chunks\": [len(header)],\n        \"dimension_separator\": r\"/\",\n        \"order\": \"F\",\n        \"dtype\": \"|u1\",\n        \"fill_value\": None,\n        \"compressor\": None,\n    }\n    omz.create_dataset(\"nifti\", data=header, shape=shape, **opt)\n    print(\"done.\")\n</code></pre>"},{"location":"docs/api/utils/__init__/","title":"init","text":"<p>Various utilities.</p>"},{"location":"docs/api/utils/j2k/","title":"J2k","text":"<p>Utilities for JPEG2000 files.</p>"},{"location":"docs/api/utils/j2k/#utils.j2k.WrappedJ2K","title":"<code>WrappedJ2K</code>  <code>dataclass</code>","text":"<p>Array-like wrapper around a JPEG2000 object.</p> <p>A wrapper around the J2K object at any resolution level, and with virtual transposition of the axes into [C, H, W] order.</p> <p>The resulting object can be sliced, but each index must be a <code>slice</code> (dropping axes using integer indices or adding axes using <code>None</code> indices is forbidden).</p> <p>The point is to ensure that the zarr writer only loads chunk-sized data.</p>"},{"location":"docs/api/utils/j2k/#utils.j2k.WrappedJ2K--parameters","title":"Parameters","text":"<p>j2k : glymur.Jp2k     The JPEG2000 object. level : int     Resolution level to map (highest resolution = 0). channel_first : bool     Return an array with shape (C, H, W) instead of (H, W, C)     when there is a channel dimension.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>@dataclass\nclass WrappedJ2K:\n    \"\"\"\n    Array-like wrapper around a JPEG2000 object.\n\n    A wrapper around the J2K object at any resolution level, and\n    with virtual transposition of the axes into [C, H, W] order.\n\n    The resulting object can be sliced, but each index must be a `slice`\n    (dropping axes using integer indices or adding axes using `None`\n    indices is forbidden).\n\n    The point is to ensure that the zarr writer only loads chunk-sized data.\n\n    Parameters\n    ----------\n    j2k : glymur.Jp2k\n        The JPEG2000 object.\n    level : int\n        Resolution level to map (highest resolution = 0).\n    channel_first : bool\n        Return an array with shape (C, H, W) instead of (H, W, C)\n        when there is a channel dimension.\n    \"\"\"\n\n    j2k: Jp2k\n    level: int = 0\n    channel_first: bool = True\n\n    @property\n    def shape(self) -&gt; tuple[int]:\n        \"\"\"Shape of the current level.\"\"\"\n        channel = list(self.j2k.shape[2:])\n        shape = [ceildiv(s, 2**self.level) for s in self.j2k.shape[:2]]\n        if self.channel_first:\n            shape = channel + shape\n        else:\n            shape += channel\n        return tuple(shape)\n\n    @property\n    def dtype(self) -&gt; np.dtype:\n        \"\"\"Data type of the wrapped image.\"\"\"\n        return self.j2k.dtype\n\n    def __getitem__(self, index: tuple[slice] | slice) -&gt; np.ndarray:\n        \"\"\"Multidimensional slicing of the wrapped array.\"\"\"\n        if not isinstance(index, tuple):\n            index = (index,)\n        if Ellipsis not in index:\n            index += (Ellipsis,)\n        if any(idx is None for idx in index):\n            raise TypeError(\"newaxis not supported\")\n\n        # substitute ellipses\n        new_index = []\n        has_seen_ellipsis = False\n        last_was_ellipsis = False\n        nb_ellipsis = max(0, self.j2k.ndim + 1 - len(index))\n        for idx in index:\n            if idx is Ellipsis:\n                if not has_seen_ellipsis:\n                    new_index += [slice(None)] * nb_ellipsis\n                elif not last_was_ellipsis:\n                    raise ValueError(\"Multiple ellipses should be contiguous\")\n                has_seen_ellipsis = True\n                last_was_ellipsis = True\n            elif not isinstance(idx, slice):\n                raise TypeError(\"Only slices are supported\")\n            elif idx.step not in (None, 1):\n                raise ValueError(\"Striding not supported\")\n            else:\n                last_was_ellipsis = False\n                new_index += [idx]\n        index = new_index\n\n        if self.channel_first:\n            *cidx, hidx, widx = index\n        else:\n            hidx, widx, *cidx = index\n        hstart, hstop = hidx.start or 0, hidx.stop or 0\n        wstart, wstop = widx.start or 0, widx.stop or 0\n\n        # convert to level 0 indices\n        hstart *= 2**self.level\n        hstop *= 2**self.level\n        wstart *= 2**self.level\n        wstop *= 2**self.level\n        hstop = min(hstop or self.j2k.shape[0], self.j2k.shape[0])\n        wstop = min(wstop or self.j2k.shape[1], self.j2k.shape[1])\n        area = (hstart, wstart, hstop, wstop)\n\n        data = self.j2k.read(rlevel=self.level, area=area)\n        if cidx:\n            data = data[:, :, cidx[0]]\n            if self.channel_first:\n                data = np.transpose(data, [2, 0, 1])\n        return data\n</code></pre>"},{"location":"docs/api/utils/j2k/#utils.j2k.WrappedJ2K.dtype","title":"<code>dtype: np.dtype</code>  <code>property</code>","text":"<p>Data type of the wrapped image.</p>"},{"location":"docs/api/utils/j2k/#utils.j2k.WrappedJ2K.shape","title":"<code>shape: tuple[int]</code>  <code>property</code>","text":"<p>Shape of the current level.</p>"},{"location":"docs/api/utils/j2k/#utils.j2k.WrappedJ2K.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Multidimensional slicing of the wrapped array.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>def __getitem__(self, index: tuple[slice] | slice) -&gt; np.ndarray:\n    \"\"\"Multidimensional slicing of the wrapped array.\"\"\"\n    if not isinstance(index, tuple):\n        index = (index,)\n    if Ellipsis not in index:\n        index += (Ellipsis,)\n    if any(idx is None for idx in index):\n        raise TypeError(\"newaxis not supported\")\n\n    # substitute ellipses\n    new_index = []\n    has_seen_ellipsis = False\n    last_was_ellipsis = False\n    nb_ellipsis = max(0, self.j2k.ndim + 1 - len(index))\n    for idx in index:\n        if idx is Ellipsis:\n            if not has_seen_ellipsis:\n                new_index += [slice(None)] * nb_ellipsis\n            elif not last_was_ellipsis:\n                raise ValueError(\"Multiple ellipses should be contiguous\")\n            has_seen_ellipsis = True\n            last_was_ellipsis = True\n        elif not isinstance(idx, slice):\n            raise TypeError(\"Only slices are supported\")\n        elif idx.step not in (None, 1):\n            raise ValueError(\"Striding not supported\")\n        else:\n            last_was_ellipsis = False\n            new_index += [idx]\n    index = new_index\n\n    if self.channel_first:\n        *cidx, hidx, widx = index\n    else:\n        hidx, widx, *cidx = index\n    hstart, hstop = hidx.start or 0, hidx.stop or 0\n    wstart, wstop = widx.start or 0, widx.stop or 0\n\n    # convert to level 0 indices\n    hstart *= 2**self.level\n    hstop *= 2**self.level\n    wstart *= 2**self.level\n    wstop *= 2**self.level\n    hstop = min(hstop or self.j2k.shape[0], self.j2k.shape[0])\n    wstop = min(wstop or self.j2k.shape[1], self.j2k.shape[1])\n    area = (hstart, wstart, hstop, wstop)\n\n    data = self.j2k.read(rlevel=self.level, area=area)\n    if cidx:\n        data = data[:, :, cidx[0]]\n        if self.channel_first:\n            data = np.transpose(data, [2, 0, 1])\n    return data\n</code></pre>"},{"location":"docs/api/utils/j2k/#utils.j2k.get_pixelsize","title":"<code>get_pixelsize(j2k)</code>","text":"<p>Read pixelsize from the JPEG2000 file.</p> Source code in <code>linc_convert/utils/j2k.py</code> <pre><code>def get_pixelsize(j2k: Jp2k) -&gt; tuple[float, float]:\n    \"\"\"Read pixelsize from the JPEG2000 file.\"\"\"\n    # Adobe XMP metadata\n    # https://en.wikipedia.org/wiki/Extensible_Metadata_Platform\n    XMP_UUID = \"BE7ACFCB97A942E89C71999491E3AFAC\"\n    TAG_Images = \"{http://ns.adobe.com/xap/1.0/}Images\"\n    Tag_Desc = \"{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Description\"\n    Tag_PixelWidth = \"{http://ns.adobe.com/xap/1.0/}PixelWidth\"\n    Tag_PixelHeight = \"{http://ns.adobe.com/xap/1.0/}PixelHeight\"\n\n    vxw = vxh = 1.0\n    for box in j2k.box:\n        if getattr(box, \"uuid\", None) == uuid.UUID(XMP_UUID):\n            try:\n                images = list(box.data.iter(TAG_Images))[0]\n                desc = list(images.iter(Tag_Desc))[0]\n                vxw = float(desc.attrib[Tag_PixelWidth])\n                vxh = float(desc.attrib[Tag_PixelHeight])\n            except Exception:\n                pass\n    return vxw, vxh\n</code></pre>"},{"location":"docs/api/utils/math/","title":"Math","text":"<p>Math utilities.</p>"},{"location":"docs/api/utils/math/#utils.math.ceildiv","title":"<code>ceildiv(x, y)</code>","text":"<p>Ceil of ratio of two numbers.</p> Source code in <code>linc_convert/utils/math.py</code> <pre><code>def ceildiv(x: Number, y: Number) -&gt; int:\n    \"\"\"Ceil of ratio of two numbers.\"\"\"\n    return int(math.ceil(x / y))\n</code></pre>"},{"location":"docs/api/utils/math/#utils.math.floordiv","title":"<code>floordiv(x, y)</code>","text":"<p>Floor of ratio of two numbers.</p> Source code in <code>linc_convert/utils/math.py</code> <pre><code>def floordiv(x: Number, y: Number) -&gt; int:\n    \"\"\"Floor of ratio of two numbers.\"\"\"\n    return int(math.floor(x / y))\n</code></pre>"},{"location":"docs/api/utils/orientation/","title":"Orientation","text":"<p>Orientation of an array of voxels with respect to world space.</p>"},{"location":"docs/api/utils/orientation/#utils.orientation.center_affine","title":"<code>center_affine(affine, shape)</code>","text":"<p>Ensure that the center of the field-of-view has world coordinate (0,0,0).</p> <p>The input affine is NOT modified in-place</p>"},{"location":"docs/api/utils/orientation/#utils.orientation.center_affine--parameters","title":"Parameters","text":"<p>affine : array     Orientation affine matrix shape : list[int]     Shape of the array of voxels</p>"},{"location":"docs/api/utils/orientation/#utils.orientation.center_affine--returns","title":"Returns","text":"<p>affine : array     Modified affine matrix.</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def center_affine(affine: np.ndarray, shape: list[int]) -&gt; np.ndarray:\n    \"\"\"\n    Ensure that the center of the field-of-view has world coordinate (0,0,0).\n\n    !!! note \"The input affine is NOT modified in-place\"\n\n    Parameters\n    ----------\n    affine : array\n        Orientation affine matrix\n    shape : list[int]\n        Shape of the array of voxels\n\n    Returns\n    -------\n    affine : array\n        Modified affine matrix.\n    \"\"\"\n    if len(shape) == 2:\n        shape = [*shape, 1]\n    shape = np.asarray(shape)\n    affine = np.copy(affine)\n    affine[:3, -1] = -0.5 * affine[:3, :3] @ (shape - 1)\n    return affine\n</code></pre>"},{"location":"docs/api/utils/orientation/#utils.orientation.orientation_ensure_3d","title":"<code>orientation_ensure_3d(orientation)</code>","text":"<p>Convert an ND orientation string to a 3D orientation string.</p>"},{"location":"docs/api/utils/orientation/#utils.orientation.orientation_ensure_3d--parameters","title":"Parameters","text":"<p>orientation : str     A 2D or 3D orientation string, such as <code>\"RA\"</code> or <code>\"RAS\"</code>.</p>"},{"location":"docs/api/utils/orientation/#utils.orientation.orientation_ensure_3d--returns","title":"Returns","text":"<p>orientation : str     A 3D orientation string compatible with the input orientaition</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def orientation_ensure_3d(orientation: str) -&gt; str:\n    \"\"\"\n    Convert an ND orientation string to a 3D orientation string.\n\n    Parameters\n    ----------\n    orientation : str\n        A 2D or 3D orientation string, such as `\"RA\"` or `\"RAS\"`.\n\n    Returns\n    -------\n    orientation : str\n        A 3D orientation string compatible with the input orientaition\n    \"\"\"\n    orientation = {\n        \"coronal\": \"LI\",\n        \"axial\": \"LP\",\n        \"sagittal\": \"PI\",\n    }.get(orientation.lower(), orientation).upper()\n    if len(orientation) == 2:\n        if \"L\" not in orientation and \"R\" not in orientation:\n            orientation += \"R\"\n        if \"P\" not in orientation and \"A\" not in orientation:\n            orientation += \"A\"\n        if \"I\" not in orientation and \"S\" not in orientation:\n            orientation += \"S\"\n    return orientation\n</code></pre>"},{"location":"docs/api/utils/orientation/#utils.orientation.orientation_to_affine","title":"<code>orientation_to_affine(orientation, vxw=1, vxh=1, vxd=1)</code>","text":"<p>Build an affine matrix from an orientation string and voxel size.</p>"},{"location":"docs/api/utils/orientation/#utils.orientation.orientation_to_affine--parameters","title":"Parameters","text":"<p>orientation : str     Orientation string vxw : float     Width voxel size vxh : float     Height voxel size vxd : float     Depth voxel size</p>"},{"location":"docs/api/utils/orientation/#utils.orientation.orientation_to_affine--returns","title":"Returns","text":"<p>affine : (4, 4) array     Affine orientation matrix</p> Source code in <code>linc_convert/utils/orientation.py</code> <pre><code>def orientation_to_affine(\n    orientation: str, vxw: float = 1, vxh: float = 1, vxd: float = 1\n) -&gt; np.ndarray:\n    \"\"\"\n    Build an affine matrix from an orientation string and voxel size.\n\n    Parameters\n    ----------\n    orientation : str\n        Orientation string\n    vxw : float\n        Width voxel size\n    vxh : float\n        Height voxel size\n    vxd : float\n        Depth voxel size\n\n    Returns\n    -------\n    affine : (4, 4) array\n        Affine orientation matrix\n    \"\"\"\n    orientation = orientation_ensure_3d(orientation)\n    affine = np.zeros([4, 4])\n    vx = np.asarray([vxw, vxh, vxd])\n    for i in range(3):\n        letter = orientation[i]\n        sign = -1 if letter in \"LPI\" else 1\n        letter = {\"L\": \"R\", \"P\": \"A\", \"I\": \"S\"}.get(letter, letter)\n        index = list(\"RAS\").index(letter)\n        affine[index, i] = sign * vx[i]\n    return affine\n</code></pre>"},{"location":"docs/api/utils/zarr/","title":"Zarr","text":"<p>Zarr utilities.</p>"},{"location":"docs/api/utils/zarr/#utils.zarr.make_compressor","title":"<code>make_compressor(name, **prm)</code>","text":"<p>Build compressor object from name and options.</p> Source code in <code>linc_convert/utils/zarr.py</code> <pre><code>def make_compressor(name: str, **prm: dict) -&gt; numcodecs.abc.Codec:\n    \"\"\"Build compressor object from name and options.\"\"\"\n    # TODO: we should use `numcodecs.get_codec` instead`\n    if not isinstance(name, str):\n        return name\n    name = name.lower()\n    if name == \"blosc\":\n        Compressor = numcodecs.Blosc\n    elif name == \"zlib\":\n        Compressor = numcodecs.Zlib\n    else:\n        raise ValueError(\"Unknown compressor\", name)\n    return Compressor(**prm)\n</code></pre>"}]}